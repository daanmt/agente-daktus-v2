# QA ENGINEERING AUDIT - AGENT SIMPLIFICATION MANDATE

## AUDIT SUMMARY

**Status:** APPROVED WITH MODIFICATIONS  
**Risk Level:** HIGH (Major Architecture Replacement)  
**Recommended Approach:** PHASED IMPLEMENTATION WITH PARALLEL VALIDATION

Product Manager's mandate for complete agent simplification is **technically sound and strategically critical**. Current system has accumulated dangerous clinical logic in code that violates separation of concerns and introduces medical interpretation risks. However, this is a complete architecture replacement in a production healthcare system requiring careful phased migration with validation gates.

---

## SPECIFICATION ANALYSIS

### Strengths

✅ **Clear Problem Identification**: PM correctly identified that code-based clinical logic (regex, heuristics, fallbacks) is dangerous and unmaintainable in healthcare context

✅ **Architectural Vision**: LLM-centric design with agent as pure connector is architecturally sound and reduces complexity

✅ **Separation of Concerns**: Proposed design properly separates clinical intelligence (LLM) from orchestration (code)

✅ **Specialty Agnostic Goal**: Making system work identically across ORL, AVC, Pediatrics, etc. is critical for scalability

✅ **Risk Awareness**: PM explicitly acknowledges stakeholder concern: "trash in, trash out" and agent introducing errors

### Critical Issues

**[ISSUE 1]: Incomplete Specification of Current System State**  
**Impact:** Cannot design safe migration without understanding exact current functionality  
**Severity:** CRITICAL  
**Required Fix:**  
```python
# Must document EXACTLY what current qa_agent.py does:
- What does _extract_exams_fallback() actually extract?
- What clinical decisions does _normalize_exam_name() make?
- What are the hardcoded ORL exam patterns?
- What quality thresholds exist?
- What triggers reexecution?
- What specialty-specific logic exists?

# Without this, we risk:
- Losing critical functionality silently
- Breaking downstream report expectations
- Introducing gaps in clinical coverage
```

**[ISSUE 2]: No Output Schema Specification**  
**Impact:** LLM output structure undefined, breaks downstream report_generator.py and semantic_analyzer.py  
**Severity:** CRITICAL  
**Required Fix:**  
```python
# Must specify EXACT JSON schema that LLM returns:
{
    "clinical_extraction": {
        "syndromes": [{"name": str, "confidence": float, "source": str}],
        "exams": [{"name": str, "indication": str, "contraindications": [str]}],
        "treatments": [{"name": str, "dosage": str, "conditions": [str]}],
        "red_flags": [{"flag": str, "urgency": str, "action": str}]
    },
    "structural_analysis": {
        "json_valid": bool,
        "syntax_errors": [str],
        "logic_issues": [{"node": str, "issue": str, "severity": str}],
        "unreachable_nodes": [str],
        "dead_end_paths": [str]
    },
    "clinical_alignment": {
        "coverage_score": float,  # 0-100
        "missing_in_protocol": [{"element": str, "type": str, "importance": str}],
        "missing_in_playbook": [{"element": str, "type": str}],
        "misalignments": [{"protocol_element": str, "playbook_element": str, "discrepancy": str}]
    },
    "recommendations": [{"priority": str, "category": str, "description": str, "impact": str}],
    "safety_assessment": {
        "critical_gaps": [{"gap": str, "risk": str, "mitigation": str}],
        "contraindication_coverage": float,
        "red_flag_coverage": float,
        "safety_score": float  # 0-100
    },
    "metadata": {
        "analysis_timestamp": str,
        "model_used": str,
        "playbook_version": str,
        "protocol_version": str
    }
}

# This schema MUST be compatible with existing report_generator.py expectations
```

**[ISSUE 3]: No LLM Response Validation Strategy**  
**Impact:** LLM may return malformed JSON, partial responses, or hallucinations  
**Severity:** HIGH  
**Required Fix:**  
```python
# Must specify validation layers:

# Layer 1: JSON Structure Validation
def validate_llm_response(response: dict) -> tuple[bool, list[str]]:
    """Validate LLM response against expected schema"""
    required_keys = ["clinical_extraction", "structural_analysis", 
                     "clinical_alignment", "recommendations", 
                     "safety_assessment", "metadata"]
    errors = []
    
    for key in required_keys:
        if key not in response:
            errors.append(f"Missing required key: {key}")
    
    # Validate sub-schemas
    # Validate data types
    # Validate value ranges
    
    return len(errors) == 0, errors

# Layer 2: Clinical Completeness Validation
def validate_clinical_completeness(response: dict) -> tuple[bool, list[str]]:
    """Ensure LLM provided meaningful clinical analysis"""
    warnings = []
    
    if len(response["clinical_extraction"]["exams"]) == 0:
        warnings.append("No exams extracted - playbook may lack exam details")
    
    if response["clinical_alignment"]["coverage_score"] < 20:
        warnings.append("Extremely low coverage - may indicate extraction failure")
    
    return len(warnings) == 0, warnings

# Layer 3: Hallucination Detection
def detect_potential_hallucinations(response: dict, playbook: str, protocol: dict) -> list[str]:
    """Flag elements in LLM response not found in source materials"""
    # Compare extracted elements against source text
    # Flag exams/treatments/syndromes not in playbook
    # Flag protocol elements not in actual JSON
    pass
```

**[ISSUE 4]: No Failure Mode Specification**  
**Impact:** System behavior undefined when LLM fails, times out, or returns invalid data  
**Severity:** HIGH  
**Required Fix:** Must specify behavior for:
- LLM API timeout → Retry? Fail? Partial results?
- Malformed JSON response → Reformat? Re-prompt? Error report?
- Empty clinical extraction → Warning? Error? Acceptable if playbook has no clinical content?
- Confidence scores below threshold → Flag? Accept with warning?
- LLM refuses analysis → How to handle refusal?

**[ISSUE 5]: No Performance Baseline**  
**Impact:** Cannot validate "acceptable time" for single LLM call  
**Severity:** MEDIUM  
**Required Fix:**
```python
# Must specify:
- Maximum acceptable latency: [X seconds]
- Token budget for super prompt: [Y tokens]
- Expected completion time percentiles:
  - p50: [N seconds]
  - p95: [M seconds]
  - p99: [K seconds]
- Timeout threshold: [T seconds]

# Current qa_agent.py performance for comparison:
# - Average end-to-end: ? seconds
# - LLM calls: ? (multiple extraction + analysis calls)
# - Total tokens: ?
```

### Ambiguities

**[AMBIGUITY 1]: Definition of "Clinical Logic"**  
**Why Unclear:** Boundary between orchestration and clinical logic not precisely defined  
**Question:** Is field extraction (e.g., extracting "age" from protocol JSON) considered clinical logic?  
**Options:**  
A) Any processing of medical content = clinical logic (ultra-strict)  
B) Only medical decision-making = clinical logic (interpretation, validation, recommendations)  
C) Medical terminology handling = clinical logic (normalization, synonym matching)  

**Recommendation:** Option B - Medical decision-making is clinical logic. Simple data extraction from JSON/text is orchestration. This allows agent to parse JSON structure while delegating all clinical interpretation to LLM.

**[AMBIGUITY 2]: Scope of "Super Prompt"**  
**Why Unclear:** Should prompt include playbook/protocol content inline or by reference?  
**Question:** How to handle large playbooks (50+ pages) that exceed LLM context window?  
**Options:**  
A) Full inline inclusion (risk: context overflow)  
B) Summarize playbook first, then analyze  
C) Chunk-based analysis with synthesis step  

**Recommendation:** Option C - For large playbooks, implement chunking strategy:
```python
if playbook_token_count > CHUNK_THRESHOLD:
    # Chunk 1: Extract clinical entities (syndromes, exams, treatments)
    # Chunk 2: Structural protocol analysis
    # Chunk 3: Alignment analysis using extracted entities
    # Synthesis: Combine results into final report
else:
    # Single comprehensive analysis
```

**[AMBIGUITY 3]: Definition of "Specialty Agnostic"**  
**Why Unclear:** Does this mean zero configuration per specialty, or identical code with different prompts?  
**Question:** Can we have specialty-specific prompt templates while keeping code identical?  
**Options:**  
A) Single universal prompt for all specialties  
B) Prompt templates per specialty, same code  
C) Configurable prompt sections loaded from external files  

**Recommendation:** Option C - Agent uses single code path, but loads specialty-agnostic prompt structure from config. This allows prompt optimization per specialty without code changes:
```python
# config/prompts.yaml
base_qa_analysis:
  clinical_extraction: "Extract all clinical elements..."
  structural_analysis: "Analyze JSON structure..."
  
# Specialty-specific emphasis (optional):
specialty_overrides:
  orl:
    additional_focus: "Pay special attention to audiometry patterns..."
  avc:
    additional_focus: "Emphasize timing of interventions..."
```

### Missing Requirements

❌ **Missing: Current Functionality Preservation Checklist**
```python
# Must document what MUST be preserved:
- [ ] Playbook extraction returns N entities (current baseline)
- [ ] Protocol validation catches M error types (enumerate)
- [ ] Semantic analysis calculates coverage using X formula
- [ ] Reports include Y sections with Z fields
- [ ] CLI interface supports all current commands
- [ ] Log format matches current structure for downstream parsing
```

❌ **Missing: LLM Model Selection Strategy**
```python
# Current system supports multiple models, spec doesn't address:
- Which model for super prompt? (Claude Sonnet 4, Gemini 2.5 Flash, Grok?)
- Model selection criteria: speed vs accuracy tradeoff
- Fallback model if primary fails
- Cost considerations
- Token limit compatibility
```

❌ **Missing: Backward Compatibility Requirements**
```python
# Must specify:
- Which existing report formats must be maintained?
- Which log formats must remain unchanged?
- Which CLI commands/flags must work identically?
- Which file formats must be supported?
- Which API interfaces (if any) must remain stable?
```

❌ **Missing: Observability Requirements**
```python
# Must specify logging for:
- LLM prompt construction (log prompt hash, token count)
- LLM API call (request_id, model, latency, tokens_used)
- LLM response validation (success/failure, validation errors)
- Clinical completeness checks (warnings, confidence scores)
- Performance metrics (per-stage timing)
- Error conditions (with full context for debugging)
```

❌ **Missing: Security Requirements**
```python
# Healthcare context requires:
- PII handling in playbook content
- Secure API key management
- Audit trail requirements
- Data retention policies
- Log sanitization (remove patient info if present)
```

❌ **Missing: Acceptance Criteria Measurements**
```python
# Criteria from PM are qualitative, need quantitative:

# "Zero heurísticas clínicas" → How to measure?
# Proposed: grep -r "regex.*audiometria|normalize.*exam" src/ returns 0 matches

# "LLM faz tudo" → How to verify?
# Proposed: All clinical decisions traceable to LLM output in logs

# "Funciona idêntico para ORL, AVC, Pediatria" → How to test?
# Proposed: Run same test protocol on 3 specialties, verify identical code path

# Need specific, measurable criteria for each of 10 acceptance points
```

---

## ARCHITECTURAL VALIDATION

### Impact Assessment

**Components Affected:**

**qa_agent.py (1074 lines)** - COMPLETE REPLACEMENT - Risk: CRITICAL
- Core monolith being replaced with modular architecture
- All extraction, validation, analysis logic removed
- Rewritten as simple orchestrator
- Risk: Total system regression if new architecture incomplete

**semantic_analyzer.py** - SCHEMA DEPENDENCY - Risk: HIGH
- Expects specific playbook_data structure from qa_agent
- Must validate LLM output schema matches expectations
- Risk: Analysis fails if schema changes

**report_generator.py** - SCHEMA DEPENDENCY - Risk: HIGH
- Expects specific analysis result structure
- Generates reports from nested dictionaries
- Risk: Report generation breaks if data structure changes

**run_qa_cli.py** - INTERFACE DEPENDENCY - Risk: MEDIUM
- Orchestrates pipeline, handles user interaction
- Must work with new simplified architecture
- Risk: CLI commands break if interface changes

**llm_client.py** - INTEGRATION POINT - Risk: LOW
- Already exists but not integrated
- New architecture should use existing client
- Risk: Minimal if properly integrated

### Integration Points

**qa_agent.py → semantic_analyzer.py**  
Data Flow: `playbook_data: dict → semantic_analysis(playbook_data, protocol_data)`  
Validation Needed:
```python
# Current expected schema:
playbook_data = {
    "syndromes": [{"name": str, "symptoms": [str]}],
    "exams": [{"name": str, "normalized_name": str}],  # ← normalized_name must be preserved or analyzer fails
    "treatments": [{"name": str, "indications": [str]}]
}

# New LLM output must match this schema exactly
# OR semantic_analyzer.py must be updated to accept new schema
```

**qa_agent.py → report_generator.py**  
Data Flow: `analysis_results: dict → generate_report(analysis_results)`  
Validation Needed:
```python
# Current expected schema:
analysis_results = {
    "coverage_analysis": {
        "score": float,
        "missing_elements": [str],
        # ... other fields
    },
    "improvement_suggestions": [
        {"priority": str, "description": str}
    ]
}

# Must verify new LLM output includes all fields report_generator expects
```

**run_qa_cli.py → qa_agent.py**  
Data Flow: `cli_args → QAAgent.run(playbook_path, protocol_path, model)`  
Validation Needed:
```python
# Current interface:
agent = QAAgent(model="claude-sonnet-4", cache_dir="./cache")
results = agent.analyze_protocol(playbook_path, protocol_path)

# New interface must maintain same method signatures
# OR cli must be updated to use new interface
```

### Regression Risks

**[REGRESSION RISK 1]: Playbook Extraction Quality**  
**Why at Risk:** Current system extracts 0 entities (known bug), but may have workarounds/fallbacks that new system lacks  
**Mitigation Strategy:**
```python
# Phase 1: Validate LLM extraction equals or exceeds current extraction
# Acceptance: LLM extracts >= N entities on test playbooks
# If LLM extraction worse, enhance prompt before proceeding
```

**[REGRESSION RISK 2]: Protocol Validation Coverage**  
**Why at Risk:** Current system may catch JSON errors that LLM doesn't explicitly validate  
**Mitigation Strategy:**
```python
# Phase 1: JSON schema validation BEFORE LLM call
# Use existing protocol_validator.py for structural checks
# LLM focuses on clinical/semantic validation only
```

**[REGRESSION RISK 3]: Report Format Compatibility**  
**Why at Risk:** Downstream tools may parse existing report format; changes break parsing  
**Mitigation Strategy:**
```python
# Phase 1: Maintain exact output format initially
# Add format_version field to detect new vs old format
# Phase 2+: Introduce new format with migration guide
```

**[REGRESSION RISK 4]: Performance Degradation**  
**Why at Risk:** Single large LLM call may be slower than current multi-step pipeline  
**Mitigation Strategy:**
```python
# Measure current system baseline
# Set timeout threshold: max(current_avg * 1.5, 60s)
# If new system slower, implement chunking strategy
```

### Architectural Concerns

**[CONCERN 1]: Single Point of Failure**  
**Description:** All intelligence in one LLM call; if it fails, entire analysis fails  
**Violates:** Robustness principle - no fallback for LLM failure  
**Consequence:** System becomes unusable during LLM outages or rate limiting  
**Recommendation:**
```python
# Add graceful degradation:
try:
    full_analysis = llm_client.analyze(super_prompt)
except LLMTimeout:
    # Fallback: Basic structural analysis only
    basic_analysis = run_basic_validation(protocol)
    return {
        "status": "partial",
        "reason": "LLM timeout",
        "basic_analysis": basic_analysis
    }
```

**[CONCERN 2]: Lack of Intermediate Validation**  
**Description:** No validation between LLM response and downstream consumption  
**Violates:** Fail-fast principle - errors propagate silently  
**Consequence:** Invalid LLM output causes cryptic failures in report generation  
**Recommendation:**
```python
# Add validation layer:
llm_response = llm_client.analyze(prompt)
validation_result = validate_llm_output(llm_response)
if not validation_result.is_valid:
    log.error(f"LLM output validation failed: {validation_result.errors}")
    # Attempt repair or fail with clear error
```

**[CONCERN 3]: Prompt Engineering as Hidden Logic**  
**Description:** Moving clinical logic to prompt doesn't eliminate it, just hides it  
**Violates:** Explainability - prompt becomes black box  
**Consequence:** Debugging requires reading 5000-token prompt; changes require prompt engineering expertise  
**Recommendation:**
```python
# Make prompt modular and versioned:
prompt = PromptBuilder(version="1.0")
prompt.add_section("clinical_extraction", template="extract_clinical.txt")
prompt.add_section("structural_analysis", template="analyze_structure.txt")
# Each section documented, versioned, and testable independently
```

### Compatibility Verification

**Schema Compatibility Analysis:**
```python
# CRITICAL: Must maintain compatibility with semantic_analyzer.py

# Current schema (from semantic_analyzer.py expectations):
CURRENT_SCHEMA = {
    "playbook_data": {
        "syndromes": [{"name": str}],
        "exams": [{"name": str, "normalized_name": str}],  # ← Key field
        "treatments": [{"name": str}]
    }
}

# New LLM output schema (from PM spec):
NEW_SCHEMA = {
    "clinical_extraction": {
        "syndromes": [{"name": str, "confidence": float}],  # Added confidence
        "exams": [{"name": str, "indication": str}],  # Missing normalized_name!
        "treatments": [{"name": str, "dosage": str}]  # Added dosage
    }
}

# INCOMPATIBILITY DETECTED:
# - "normalized_name" field missing in new schema
# - "exams" structure changed (indication vs normalized_name)

# RESOLUTION OPTIONS:
# A) Update semantic_analyzer.py to accept new schema (HIGH RISK)
# B) Add transformation layer: new_schema → old_schema (MEDIUM RISK)
# C) Enhance LLM prompt to include normalized_name field (LOW RISK - RECOMMENDED)

# Recommended: Option C
FINAL_SCHEMA = {
    "clinical_extraction": {
        "exams": [
            {
                "name": str,              # Original exam name
                "normalized_name": str,   # Preserved for compatibility
                "indication": str,        # New field
                "confidence": float       # New field
            }
        ]
    }
}
```

**Interface Compatibility Analysis:**
```python
# Current qa_agent.py interface:
class QAAgent:
    def __init__(self, model: str, cache_dir: str):
        pass
    
    def analyze_protocol(self, playbook_path: str, protocol_path: str) -> dict:
        pass

# New simplified agent (from PM spec):
class SimplifiedQARunner:
    def run_analysis(self, playbook_path: str, protocol_path: str) -> dict:
        pass

# INCOMPATIBILITY DETECTED:
# - Different class name
# - Different method name
# - Missing __init__ parameters

# RESOLUTION:
# Maintain interface compatibility with wrapper:
class QAAgent:  # Keep existing name
    def __init__(self, model: str = "claude-sonnet-4", cache_dir: str = None):
        self.runner = SimplifiedQARunner(model=model)
    
    def analyze_protocol(self, playbook_path: str, protocol_path: str) -> dict:
        # Delegate to new implementation
        return self.runner.run_analysis(playbook_path, protocol_path)
```

**Behavioral Compatibility Analysis:**
```python
# Current behavior: Multiple LLM calls with caching
# - Call 1: Extract playbook entities
# - Call 2: Validate protocol structure
# - Call 3: Semantic analysis
# - Call 4: Improvement suggestions

# New behavior: Single LLM call
# - Single comprehensive analysis

# BEHAVIORAL CHANGE:
# - Caching strategy changes (was per-call, now per-analysis)
# - Error isolation changes (was per-stage, now all-or-nothing)
# - Progress visibility changes (was per-stage logs, now single operation)

# IMPACT ASSESSMENT:
# - Performance: May be faster (fewer API calls) or slower (larger prompt)
# - Observability: Less granular progress tracking
# - Debuggability: Harder to isolate which stage failed

# MITIGATION:
# - Add timing markers within LLM response
# - Log prompt construction stages
# - Maintain cache at analysis level, not stage level
```

---

## IMPLEMENTATION VALIDATION

### Testability Assessment

**Unit Tests Required:**

```python
def test_content_loader_loads_valid_playbook():
    """
    Validates: ContentLoader correctly reads playbook file
    Given: Valid markdown playbook file exists
    When: load_playbook() called
    Then: Returns string content matching file
    """
    # Verify: File reading, encoding handling, error on missing file

def test_content_loader_loads_valid_protocol():
    """
    Validates: ContentLoader correctly parses JSON protocol
    Given: Valid JSON protocol file exists
    When: load_protocol() called
    Then: Returns dict matching JSON structure
    """
    # Verify: JSON parsing, error on malformed JSON, error on missing file

def test_prompt_builder_creates_valid_prompt():
    """
    Validates: PromptBuilder generates well-formed analysis prompt
    Given: Sample playbook content and protocol JSON
    When: build_qa_analysis_prompt() called
    Then: Returns prompt with all required sections
    """
    # Verify: Template substitution, section inclusion, token count reasonable

def test_llm_client_validates_response_schema():
    """
    Validates: LLMClient rejects malformed LLM responses
    Given: LLM returns incomplete JSON
    When: _parse_json_response() called
    Then: Raises ValidationError with specific error
    """
    # Verify: Schema validation, error messages, no silent failures

def test_qa_runner_handles_missing_files():
    """
    Validates: QARunner fails gracefully on missing input files
    Given: Playbook or protocol file doesn't exist
    When: run_analysis() called
    Then: Raises FileNotFoundError with clear message
    """
    # Verify: File existence check, error message clarity, no partial execution
```

**Integration Tests Required:**

```python
def test_end_to_end_orl_protocol_analysis():
    """
    Validates: Complete analysis pipeline for ORL specialty
    Tests integration between: ContentLoader → PromptBuilder → LLMClient → OutputFormatter
    Given: Real ORL playbook + protocol from test_data/
    When: run_analysis() executed
    Then: Returns complete analysis with all required fields
    """
    # Verify: All components work together, output schema correct, no errors

def test_end_to_end_avc_protocol_analysis():
    """
    Validates: System works identically for different specialty
    Tests integration between: Same pipeline as test_end_to_end_orl
    Given: Real AVC playbook + protocol
    When: run_analysis() executed
    Then: Returns analysis with same structure as ORL
    """
    # Verify: Specialty-agnostic behavior, identical code path

def test_llm_response_to_report_generation():
    """
    Validates: LLM output compatible with existing report_generator
    Tests integration between: SimplifiedQARunner → report_generator.py
    Given: LLM analysis output from new system
    When: report_generator.generate_report() called
    Then: Report generates successfully without errors
    """
    # Verify: Schema compatibility, no missing fields, report format correct

def test_cli_integration_with_new_agent():
    """
    Validates: CLI interface works with new agent architecture
    Tests integration between: run_qa_cli.py → QAAgent (wrapper) → SimplifiedQARunner
    Given: CLI invoked with test playbook + protocol
    When: CLI executes analysis command
    Then: Analysis completes and displays results
    """
    # Verify: CLI commands work, output formatting correct, errors displayed properly
```

**Regression Tests Required:**

```python
def test_playbook_extraction_count_baseline():
    """
    Ensures: New system extracts at least as many entities as current
    Given: Baseline test playbook with known N syndromes, M exams, K treatments
    When: New system analyzes playbook
    Then: Extracts >= N syndromes, >= M exams, >= K treatments
    """
    # Verify: No regression in extraction quality

def test_protocol_validation_error_detection():
    """
    Ensures: New system catches same JSON errors as current system
    Given: Test protocol with known syntax errors, logic errors
    When: New system analyzes protocol
    Then: Reports all errors that current system reports
    """
    # Verify: Error detection not degraded

def test_report_format_unchanged():
    """
    Ensures: Generated reports maintain existing format
    Given: Standard test protocol
    When: New system generates report
    Then: Report structure matches existing format exactly
    """
    # Verify: Downstream compatibility maintained

def test_performance_not_degraded():
    """
    Ensures: New system completes analysis within acceptable time
    Given: Baseline performance metrics from current system
    When: New system analyzes same protocols
    Then: Completion time <= baseline * 1.5
    """
    # Verify: Performance acceptable
```

**Test Coverage Gaps:**

❌ **Large Playbook Handling (>50 pages)**  
Scenario: Playbook exceeds LLM context window  
Gap: No test for chunking strategy or graceful degradation  
Required Test: Verify system chunks large playbooks and synthesizes results

❌ **LLM Timeout Recovery**  
Scenario: LLM API call times out or rate limited  
Gap: No test for retry logic or fallback behavior  
Required Test: Mock LLM timeout, verify system handles gracefully

❌ **Malformed LLM JSON Response**  
Scenario: LLM returns valid text but invalid JSON structure  
Gap: No test for JSON repair or error reporting  
Required Test: Verify system detects and reports JSON parsing failures

❌ **Concurrent Analysis Requests**  
Scenario: Multiple CLI instances running simultaneously  
Gap: No test for resource contention or cache conflicts  
Required Test: Verify thread safety and file locking

### Error Handling Validation

**Failure Modes Addressed:**

✅ **File Not Found**  
How Handled:
```python
def load_playbook(self, file_path: str) -> str:
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Playbook not found: {file_path}")
```
Validation Method: Unit test with missing file

✅ **Invalid JSON Protocol**  
How Handled:
```python
def load_protocol(self, file_path: str) -> dict:
    try:
        return json.load(f)
    except json.JSONDecodeError as e:
        raise ValueError(f"Invalid JSON in protocol: {e}")
```
Validation Method: Unit test with malformed JSON

✅ **Empty Playbook Content**  
How Handled:
```python
def validate_inputs(self, playbook: str, protocol: dict):
    if not playbook.strip():
        raise ValueError("Playbook is empty")
```
Validation Method: Unit test with empty file

**Failure Modes NOT Addressed:**

❌ **LLM Returns Partial Response**  
Why Concerning: LLM may return incomplete JSON due to token limit or streaming issues  
Required Handling:
```python
def _parse_json_response(self, response: str) -> dict:
    try:
        data = json.loads(response)
    except json.JSONDecodeError:
        # Attempt to extract JSON from markdown code blocks
        data = self._extract_json_from_markdown(response)
        if not data:
            raise LLMResponseError("Could not parse JSON from LLM response")
    
    # Validate completeness
    is_complete, missing = self._validate_response_completeness(data)
    if not is_complete:
        raise LLMResponseError(f"Incomplete LLM response, missing: {missing}")
    
    return data
```

❌ **LLM API Rate Limiting**  
Why Concerning: OpenRouter may rate limit requests, causing immediate failures  
Required Handling:
```python
def analyze(self, prompt: str, max_retries: int = 3) -> dict:
    for attempt in range(max_retries):
        try:
            return self._call_llm(prompt)
        except RateLimitError as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # Exponential backoff
                log.warning(f"Rate limited, retrying in {wait_time}s...")
                time.sleep(wait_time)
            else:
                raise LLMError("Max retries exceeded due to rate limiting")
```

❌ **LLM Hallucination of Non-Existent Clinical Elements**  
Why Concerning: LLM may invent exams/treatments not in playbook  
Required Handling:
```python
def validate_against_source(self, llm_output: dict, playbook: str) -> list[str]:
    """Flag elements in LLM output not found in source playbook"""
    warnings = []
    
    for exam in llm_output["clinical_extraction"]["exams"]:
        exam_name = exam["name"].lower()
        if exam_name not in playbook.lower():
            warnings.append(f"Potential hallucination: exam '{exam['name']}' not found in playbook")
    
    # Log warnings but don't fail - LLM may use synonyms
    for warning in warnings:
        log.warning(warning)
    
    return warnings
```

❌ **Protocol JSON Too Large for LLM Context**  
Why Concerning: Protocols with 100+ nodes may exceed context window  
Required Handling:
```python
def build_qa_analysis_prompt(self, playbook: str, protocol: dict) -> str:
    playbook_tokens = self._estimate_tokens(playbook)
    protocol_tokens = self._estimate_tokens(json.dumps(protocol))
    
    if playbook_tokens + protocol_tokens > MAX_CONTEXT_TOKENS:
        # Implement chunking strategy
        return self._build_chunked_analysis_prompt(playbook, protocol)
    else:
        return self._build_single_analysis_prompt(playbook, protocol)
```

### Observability Validation

**Logging Requirements:**

**Log Location:** `logs/qa_agent.log`  
**Log Format:** JSON (structured logging)

**Required Log Entries:**

```python
# On analysis start:
log.info({
    "timestamp": "2025-01-28T14:30:00Z",
    "event": "analysis_started",
    "context": {
        "playbook_path": "/path/to/playbook.md",
        "protocol_path": "/path/to/protocol.json",
        "playbook_size_bytes": 45000,
        "protocol_nodes": 23
    }
})

# On prompt construction:
log.debug({
    "timestamp": "2025-01-28T14:30:01Z",
    "event": "prompt_constructed",
    "context": {
        "prompt_hash": "abc123...",  # For cache/deduplication
        "prompt_token_count": 4500,
        "template_version": "1.0"
    }
})

# On LLM call:
log.info({
    "timestamp": "2025-01-28T14:30:02Z",
    "event": "llm_call_started",
    "context": {
        "model": "anthropic/claude-3.5-sonnet",
        "request_id": "req_xyz789",
        "estimated_tokens": 5000
    }
})

# On LLM response:
log.info({
    "timestamp": "2025-01-28T14:30:15Z",
    "event": "llm_call_completed",
    "context": {
        "request_id": "req_xyz789",
        "latency_ms": 13000,
        "tokens_used": 4800,
        "response_size_bytes": 12000
    },
    "outcome": "success"
})

# On validation:
log.info({
    "timestamp": "2025-01-28T14:30:16Z",
    "event": "response_validated",
    "context": {
        "schema_valid": true,
        "completeness_score": 0.95,
        "warnings": ["Low confidence on exam X"]
    }
})

# On analysis complete:
log.info({
    "timestamp": "2025-01-28T14:30:17Z",
    "event": "analysis_completed",
    "context": {
        "total_duration_ms": 17000,
        "entities_extracted": {
            "syndromes": 5,
            "exams": 12,
            "treatments": 8
        },
        "coverage_score": 78.5
    },
    "outcome": "success"
})

# On error:
log.error({
    "timestamp": "2025-01-28T14:30:05Z",
    "event": "analysis_failed",
    "context": {
        "playbook_path": "/path/to/playbook.md",
        "protocol_path": "/path/to/protocol.json",
        "stage": "llm_call",
        "error_type": "LLMTimeout",
        "error_message": "Request timed out after 30s"
    },
    "outcome": "failure",
    "stack_trace": "..."
})
```

**Log Levels:**
- **DEBUG:** Prompt construction details, token counts, cache hits
- **INFO:** Analysis stages, LLM calls, validation results, completion
- **WARNING:** Low confidence scores, potential hallucinations, timeouts with retry
- **ERROR:** LLM failures, validation failures, file errors
- **CRITICAL:** System-level failures, config errors, unrecoverable states

**Monitoring Requirements:**

```python
# Metric: Analysis Success Rate
success_rate = successful_analyses / total_analyses
threshold = 0.95
action = "Alert if < 95% over 1 hour window"

# Metric: Average Analysis Duration
avg_duration_ms = sum(durations) / count(analyses)
threshold = 30000  # 30 seconds
action = "Alert if p95 > 60s"

# Metric: LLM API Error Rate
llm_error_rate = llm_errors / llm_calls
threshold = 0.05
action = "Alert if > 5%"

# Metric: Validation Failure Rate
validation_failure_rate = validation_failures / total_analyses
threshold = 0.10
action = "Alert if > 10% - indicates prompt quality issues"
```

**Missing Observability:**

❌ **Cost Tracking**  
Gap: No logging of API costs per analysis  
Why Needed: Healthcare system needs cost visibility for budgeting  
Required:
```python
log.info({
    "event": "cost_tracked",
    "context": {
        "tokens_used": 4800,
        "estimated_cost_usd": 0.024,  # Based on model pricing
        "model": "claude-sonnet-4"
    }
})
```

❌ **Quality Trend Tracking**  
Gap: No metrics on analysis quality over time  
Why Needed: Detect prompt degradation or LLM quality changes  
Required:
```python
log.info({
    "event": "quality_metrics",
    "context": {
        "coverage_score": 78.5,
        "confidence_avg": 0.85,
        "entities_per_analysis": 25,
        "hallucination_warnings": 2
    }
})
```

---

## RISK ANALYSIS

### Likelihood × Impact Matrix

| Risk | Likelihood | Impact | Severity | Mitigation |
|------|-----------|--------|----------|------------|
| LLM returns malformed JSON | HIGH | HIGH | **CRITICAL** | Add JSON validation + repair logic |
| Downstream reports break due to schema change | MEDIUM | HIGH | **HIGH** | Maintain schema compatibility + regression tests |
| Performance degradation (>2x slower) | MEDIUM | MEDIUM | **MEDIUM** | Measure baseline + implement timeout |
| Large playbooks exceed context window | MEDIUM | HIGH | **HIGH** | Implement chunking strategy |
| LLM hallucinations introduce clinical errors | LOW | HIGH | **MEDIUM** | Add hallucination detection |
| API rate limiting causes failures | MEDIUM | MEDIUM | **MEDIUM** | Add retry logic with exponential backoff |
| Silent failure in semantic_analyzer | LOW | HIGH | **MEDIUM** | Add schema validation before passing data |
| Cost escalation from larger prompts | LOW | MEDIUM | **LOW** | Track costs + set budget alerts |
| Concurrent execution causes cache corruption | LOW | MEDIUM | **LOW** | Implement file locking |
| Rollback difficulty if migration fails | LOW | CRITICAL | **HIGH** | Phased implementation with feature flag |

### Unmitigated Risks

**[UNMITIGATED RISK 1]: Unknown Current System Behaviors**  
Why Concerning: Without complete understanding of current system, may lose undocumented functionality  
Recommendation:
```python
# Before any implementation:
1. Document all current system behaviors with test cases
2. Run comprehensive test suite on current system
3. Establish baseline metrics (entities extracted, errors caught, performance)
4. Use baselines as acceptance criteria for new system
```

**[UNMITIGATED RISK 2]: LLM Model Dependency**  
Why Concerning: System completely dependent on single LLM provider; vendor lock-in  
Recommendation:
```python
# Add abstraction layer:
class LLMProvider(ABC):
    @abstractmethod
    def analyze(self, prompt: str) -> dict:
        pass

class ClaudeProvider(LLMProvider):
    def analyze(self, prompt: str) -> dict:
        # Claude-specific implementation
        pass

class GeminiProvider(LLMProvider):
    def analyze(self, prompt: str) -> dict:
        # Gemini-specific implementation (fallback)
        pass

# Allow switching providers without code changes
```

### Risk Mitigation Strategies

**[RISK]: LLM Returns Malformed JSON**  
**Mitigation Strategy:**
```python
def _parse_json_response(self, response: str) -> dict:
    # Strategy 1: Direct JSON parsing
    try:
        return json.loads(response)
    except json.JSONDecodeError:
        pass
    
    # Strategy 2: Extract from markdown code blocks
    json_match = re.search(r'```json\s*(\{.*\})\s*```', response, re.DOTALL)
    if json_match:
        try:
            return json.loads(json_match.group(1))
        except json.JSONDecodeError:
            pass
    
    # Strategy 3: Attempt JSON repair
    try:
        return json_repair.loads(response)
    except:
        pass
    
    # Fail with detailed error
    raise LLMResponseError(
        "Could not parse JSON from LLM response",
        context={"response_preview": response[:500]}
    )
```
**Validation Method:** Unit test with various malformed responses

**[RISK]: Downstream Reports Break**  
**Mitigation Strategy:**
```python
# Add transformation layer:
class SchemaAdapter:
    """Transforms new LLM output to legacy format for compatibility"""
    
    def transform_to_legacy(self, llm_output: dict) -> dict:
        """Convert new schema to format expected by report_generator"""
        return {
            "playbook_data": {
                "syndromes": [
                    {"name": item["name"]} 
                    for item in llm_output["clinical_extraction"]["syndromes"]
                ],
                "exams": [
                    {
                        "name": item["name"],
                        "normalized_name": item.get("normalized_name", item["name"])
                    }
                    for item in llm_output["clinical_extraction"]["exams"]
                ],
                # ... transform other fields
            },
            "coverage_analysis": {
                "score": llm_output["clinical_alignment"]["coverage_score"],
                # ... map other fields
            }
        }
```
**Validation Method:** Integration test with actual report_generator.py

**[RISK]: Large Playbooks Exceed Context**  
**Mitigation Strategy:**
```python
def _build_analysis_prompt(self, playbook: str, protocol: dict) -> Union[str, list[str]]:
    """Build prompt, chunking if necessary"""
    
    MAX_TOKENS = 100000  # Model context limit
    PROMPT_OVERHEAD = 2000  # Tokens for instructions
    
    playbook_tokens = self._estimate_tokens(playbook)
    protocol_tokens = self._estimate_tokens(json.dumps(protocol))
    total_tokens = playbook_tokens + protocol_tokens + PROMPT_OVERHEAD
    
    if total_tokens <= MAX_TOKENS:
        # Single prompt
        return self._build_single_prompt(playbook, protocol)
    else:
        # Chunked analysis
        return self._build_chunked_prompts(playbook, protocol, MAX_TOKENS)

def _build_chunked_prompts(self, playbook: str, protocol: dict, max_tokens: int) -> list[str]:
    """Split analysis into multiple prompts with synthesis"""
    return [
        # Chunk 1: Extract clinical entities
        self._build_extraction_prompt(playbook),
        # Chunk 2: Analyze protocol structure
        self._build_structural_prompt(protocol),
        # Chunk 3: Align entities with protocol
        self._build_alignment_prompt("{extracted_entities}", protocol),
        # Chunk 4: Synthesize final report
        self._build_synthesis_prompt("{all_previous_results}")
    ]
```
**Validation Method:** Test with 100+ page playbook

---

## PHASED IMPLEMENTATION RECOMMENDATION

Given the **HIGH RISK** of complete architecture replacement in production healthcare system, recommend **3-PHASE IMPLEMENTATION** with validation gates:

### Phase 1: Foundation + Parallel Validation - Risk: LOW

**Goal:** Build new simplified architecture alongside existing system; validate output equivalence

**Implementation:**

```python
# Step 1: Create new simplified modules (no integration yet)
src/
├── agent_v2/  # New architecture, isolated
│   ├── __init__.py
│   ├── loader.py
│   ├── prompt_builder.py
│   ├── llm_client.py
│   ├── qa_runner.py
│   └── output/
│       ├── formatter.py
│       └── logger.py
├── agent/  # Existing qa_agent.py (unchanged)
│   └── qa_agent.py
└── config/
    ├── prompts_v2.py  # Super prompt templates
    └── feature_flags.py

# Step 2: Implement new modules with zero integration
class SimplifiedQARunner:
    """New implementation, standalone"""
    def run_analysis(self, playbook_path: str, protocol_path: str) -> dict:
        loader = ContentLoader()
        playbook = loader.load_playbook(playbook_path)
        protocol = loader.load_protocol(protocol_path)
        
        prompt_builder = PromptBuilder()
        prompt = prompt_builder.build_qa_analysis_prompt(playbook, protocol)
        
        llm_client = LLMClient()
        analysis = llm_client.analyze(prompt)
        
        formatter = OutputFormatter()
        return formatter.format_analysis_output(analysis)

# Step 3: Add parallel execution mode (feature flag)
class ParallelValidator:
    """Run both old and new system, compare outputs"""
    def validate_equivalence(self, playbook_path: str, protocol_path: str):
        # Run old system
        old_agent = QAAgent()
        old_result = old_agent.analyze_protocol(playbook_path, protocol_path)
        
        # Run new system
        new_runner = SimplifiedQARunner()
        new_result = new_runner.run_analysis(playbook_path, protocol_path)
        
        # Compare results
        comparison = self._compare_results(old_result, new_result)
        self._log_comparison(comparison)
        
        return comparison

# Step 4: Run parallel validation on test suite
python run_parallel_validation.py --test-suite tests/data/protocols/

# Step 5: Analyze validation results
- Entity extraction count (old vs new)
- Coverage score delta
- Performance comparison
- Error detection comparison
```

**Validation:**
- [ ] All new modules pass unit tests
- [ ] New system analyzes 10+ test protocols without errors
- [ ] Output schema matches expected structure
- [ ] No integration with existing system yet (zero regression risk)

**Rollback Plan:**
- Delete `agent_v2/` directory
- No changes to existing system required

**Acceptance Criteria:**
- New system extracts >= 90% of entities extracted by old system
- New system completes analysis in <= 2x time of old system
- New system output schema validated against semantic_analyzer expectations
- Zero syntax/parsing errors in new system on test suite

---

### Phase 2: Integration + Schema Compatibility - Risk: MEDIUM

**Goal:** Integrate new system with existing pipeline while maintaining backward compatibility

**Dependencies:** Phase 1 complete and validated (validation results show >= 90% equivalence)

**Implementation:**

```python
# Step 1: Add schema compatibility layer
class SchemaAdapter:
    """Transform new LLM output to legacy format"""
    def to_legacy_format(self, llm_output: dict) -> dict:
        """Convert new schema → old schema for existing components"""
        return {
            "playbook_data": self._transform_playbook_data(llm_output),
            "coverage_analysis": self._transform_coverage_data(llm_output),
            "improvement_suggestions": self._transform_recommendations(llm_output)
        }

# Step 2: Create compatibility wrapper
class QAAgent:
    """Wrapper maintaining existing interface, delegating to new implementation"""
    def __init__(self, model: str = "claude-sonnet-4", cache_dir: str = None):
        self.use_new_architecture = os.getenv("USE_SIMPLIFIED_AGENT", "false") == "true"
        
        if self.use_new_architecture:
            self.runner = SimplifiedQARunner(model=model)
            self.adapter = SchemaAdapter()
        else:
            # Legacy implementation
            self._init_legacy(model, cache_dir)
    
    def analyze_protocol(self, playbook_path: str, protocol_path: str) -> dict:
        if self.use_new_architecture:
            new_result = self.runner.run_analysis(playbook_path, protocol_path)
            return self.adapter.to_legacy_format(new_result)
        else:
            return self._legacy_analyze(playbook_path, protocol_path)

# Step 3: Update CLI to support feature flag
# run_qa_cli.py
if __name__ == "__main__":
    # Feature flag: USE_SIMPLIFIED_AGENT=true enables new architecture
    agent = QAAgent(model=args.model)
    results = agent.analyze_protocol(args.playbook, args.protocol)

# Step 4: Test integration with downstream components
def test_integration_with_semantic_analyzer():
    """Verify new system output works with semantic_analyzer.py"""
    agent = QAAgent()  # With USE_SIMPLIFIED_AGENT=true
    result = agent.analyze_protocol("test_playbook.md", "test_protocol.json")
    
    # Should not raise errors
    semantic_analyzer.analyze_coverage(result["playbook_data"], result["coverage_analysis"])

def test_integration_with_report_generator():
    """Verify new system output works with report_generator.py"""
    agent = QAAgent()
    result = agent.analyze_protocol("test_playbook.md", "test_protocol.json")
    
    # Should generate valid report
    report = report_generator.generate_report(result)
    assert report is not None
    assert "coverage_score" in report

# Step 5: Deploy with feature flag OFF (new code present, not active)
git commit -m "Add simplified agent with feature flag (inactive)"
git push

# Step 6: Enable for subset of analyses
USE_SIMPLIFIED_AGENT=true python run_qa_cli.py --playbook test.md --protocol test.json

# Step 7: Monitor logs, compare results
tail -f logs/qa_agent.log | grep "architecture=simplified"
```

**Validation:**
- [ ] New system integrated without breaking existing functionality
- [ ] Feature flag toggles between old/new architecture successfully
- [ ] Downstream components (semantic_analyzer, report_generator) work with new output
- [ ] No errors when new architecture enabled for test cases

**Rollback Plan:**
```bash
# If integration issues:
export USE_SIMPLIFIED_AGENT=false  # Instant rollback
# System reverts to legacy architecture immediately
```

**Acceptance Criteria:**
- All existing tests pass with new architecture enabled
- Reports generated from new architecture match format of old reports
- No schema validation errors in downstream components
- Performance degradation < 50%

---

### Phase 3: Full Migration + Legacy Removal - Risk: MEDIUM

**Goal:** Make new architecture default, remove legacy code

**Dependencies:** Phase 2 complete and validated (new architecture stable in production for 1+ week)

**Implementation:**

```python
# Step 1: Make new architecture default
# config/feature_flags.py
USE_SIMPLIFIED_AGENT = os.getenv("USE_SIMPLIFIED_AGENT", "true")  # Default changed to true

# Step 2: Announce deprecation
# CHANGELOG.md
## Version 2.0.0
### Breaking Changes
- Agent architecture simplified to LLM-centric design
- Legacy complex extraction pipeline deprecated
- To temporarily use legacy architecture: `export USE_SIMPLIFIED_AGENT=false`
- Legacy architecture will be removed in version 2.1.0

# Step 3: Monitor production for 1 week
# Check metrics:
- Analysis success rate (target: >= 95%)
- Average duration (target: <= 60s p95)
- Error rate (target: < 5%)
- Report generation success (target: 100%)

# Step 4: If stable, remove legacy code
git rm src/agent/qa_agent_legacy.py
git rm src/agent/_extract_exams_fallback.py
git rm src/agent/_normalize_exam_name.py
# Remove all clinical heuristics, regex patterns, fallbacks

# Step 5: Clean up compatibility layer
# No longer need SchemaAdapter - new schema is primary
# Update semantic_analyzer.py to natively accept new schema
# Update report_generator.py to natively accept new schema

# Step 6: Update documentation
# README.md - document new architecture
# docs/architecture.md - LLM-centric design
# docs/prompts.md - super prompt documentation

# Step 7: Final validation
python -m pytest tests/ --cov=src --cov-report=html
# Target: 80% code coverage, all tests pass
```

**Validation:**
- [ ] New architecture default for 1+ week with stable metrics
- [ ] Legacy code removed completely
- [ ] All tests updated for new architecture
- [ ] Documentation reflects new architecture

**Rollback Plan:**
```bash
# If catastrophic failure:
git revert <commit-hash>  # Restore legacy code
export USE_SIMPLIFIED_AGENT=false
# Return to Phase 2 state
```

**Acceptance Criteria:**
- Zero clinical heuristics in codebase: `grep -r "regex.*audiometria|normalize.*exam|_extract.*fallback" src/` returns 0 matches
- System works identically across 3+ specialties (ORL, AVC, Pediatria)
- All 10 PM acceptance criteria met (documented in test suite)
- Code coverage >= 80%
- Performance acceptable (p95 latency <= 60s)

---

## FINAL SPECIFICATION FOR SOFTWARE ENGINEER

### IMPLEMENTATION TASK: Simplified LLM-Centric QA Agent Architecture

**PRIORITY:** P0-CRITICAL  
**ESTIMATED COMPLEXITY:** HIGH (Complete Architecture Replacement)  
**RISK LEVEL:** HIGH (Production Healthcare System)

---

### CONTEXT

**Current System State:**
- Monolithic `qa_agent.py` (1074 lines) with mixed clinical and orchestration logic
- Code-based clinical extraction using regex patterns: `audiometria\s+(?:tonal|vocal)`, `timpanometria|impedanciometria`
- Medical terminology normalization via Python dictionaries
- Specialty-specific validation logic hardcoded for ORL
- Heuristic quality thresholds triggering reexecution
- Multiple LLM calls with complex caching strategy
- Known issues: 0 entity extraction, incorrect 0% coverage scores

**Problem:**
Current agent violates separation of concerns by making clinical decisions through code rather than medical expertise. This creates:
- **Safety Risk:** Code making medical interpretations without clinical validation
- **Maintainability Risk:** Clinical logic scattered across 1000+ lines
- **Scalability Risk:** Each specialty requires new hardcoded rules
- **Reliability Risk:** Heuristics introducing "trash" into validation pipeline

**Why This Matters:**
Healthcare system where failures affect clinical decision-making. Code-based medical logic is unmaintainable, clinically inappropriate, and introduces errors that LLM-based analysis avoids. Stakeholder explicitly concerned about "trash in, trash out" and agent introducing garbage.

**Related Components:**
- `semantic_analyzer.py`: Expects specific `playbook_data` schema with `normalized_name` field
- `report_generator.py`: Expects specific `analysis_results` structure for report generation
- `run_qa_cli.py`: Orchestrates pipeline, must maintain CLI interface compatibility
- `llm_client.py`: Already exists but not integrated, use for LLM communication

---

### OBJECTIVES

**Primary Goal:**
Transform agent from complex code-based clinical analyzer to simple LLM-centric orchestrator where all clinical intelligence resides in prompt + Claude, zero clinical logic in code.

**Secondary Goals:**
- Maintain schema compatibility with downstream components (semantic_analyzer, report_generator)
- Preserve CLI interface for seamless user experience
- Improve reliability by eliminating code-based clinical heuristics
- Enable specialty-agnostic operation (ORL, AVC, Pediatrics work identically)
- Maintain or improve analysis quality vs current system

**Non-Goals:**
- Do NOT update semantic_analyzer.py or report_generator.py in this phase
- Do NOT change CLI interface or user-facing commands
- Do NOT optimize prompt engineering (use comprehensive prompt, optimize later)
- Do NOT implement advanced features (caching, async, streaming) initially

---

### FUNCTIONAL REQUIREMENTS

**[FR-1]: Simple Content Loading**  
**Description:** Load playbook and protocol files without any interpretation or transformation  
**Validation Method:**
```python
def test_load_playbook_returns_raw_content():
    loader = ContentLoader()
    content = loader.load_playbook("test_playbook.md")
    assert isinstance(content, str)
    assert len(content) > 0
    assert content == open("test_playbook.md").read()  # Exact match
```
**Acceptance Criterion:** Loaded content byte-for-byte identical to file content

**[FR-2]: Unified Analysis Prompt Construction**  
**Description:** Build single comprehensive prompt containing all QA instructions, playbook content, and protocol JSON  
**Validation Method:**
```python
def test_prompt_contains_all_required_sections():
    builder = PromptBuilder()
    prompt = builder.build_qa_analysis_prompt(playbook, protocol)
    
    assert "CLINICAL CONTENT EXTRACTION" in prompt
    assert "STRUCTURAL PROTOCOL ANALYSIS" in prompt
    assert "CLINICAL-PROTOCOL ALIGNMENT" in prompt
    assert "SAFETY VALIDATION" in prompt
    assert playbook in prompt
    assert json.dumps(protocol) in prompt
```
**Acceptance Criterion:** Prompt includes all 5 analysis sections + complete source materials

**[FR-3]: LLM Analysis Execution**  
**Description:** Send prompt to Claude via existing llm_client, receive structured JSON response  
**Validation Method:**
```python
def test_llm_returns_structured_analysis():
    client = LLMClient(model="claude-sonnet-4")
    response = client.analyze(prompt)
    
    assert isinstance(response, dict)
    assert "clinical_extraction" in response
    assert "structural_analysis" in response
    assert "clinical_alignment" in response
```
**Acceptance Criterion:** LLM response is valid JSON with all required top-level keys

**[FR-4]: Response Schema Validation**  
**Description:** Validate LLM output matches expected schema before returning to caller  
**Validation Method:**
```python
def test_invalid_llm_response_raises_error():
    validator = ResponseValidator()
    
    invalid_response = {"clinical_extraction": {}}  # Missing other keys
    with pytest.raises(ValidationError) as exc:
        validator.validate(invalid_response)
    
    assert "Missing required keys" in str(exc.value)
```
**Acceptance Criterion:** Invalid responses raise ValidationError with specific missing field details

**[FR-5]: Schema Compatibility Transformation**  
**Description:** Transform new LLM output to legacy format expected by semantic_analyzer and report_generator  
**Validation Method:**
```python
def test_adapter_produces_legacy_schema():
    adapter = SchemaAdapter()
    llm_output = get_sample_llm_output()
    
    legacy_format = adapter.to_legacy_format(llm_output)
    
    # Must have fields expected by semantic_analyzer
    assert "playbook_data" in legacy_format
    assert "syndromes" in legacy_format["playbook_data"]
    assert "exams" in legacy_format["playbook_data"]
    assert "normalized_name" in legacy_format["playbook_data"]["exams"][0]
```
**Acceptance Criterion:** Transformed output passes existing semantic_analyzer and report_generator without errors

**[FR-6]: Interface Compatibility Wrapper**  
**Description:** Maintain existing QAAgent class interface while delegating to new SimplifiedQARunner  
**Validation Method:**
```python
def test_qa_agent_interface_unchanged():
    # Existing code should work without modification
    agent = QAAgent(model="claude-sonnet-4", cache_dir="./cache")
    result = agent.analyze_protocol("playbook.md", "protocol.json")
    
    assert isinstance(result, dict)
    assert "playbook_data" in result
    assert "coverage_analysis" in result
```
**Acceptance Criterion:** Existing CLI and test code works without modification when new architecture enabled

**[FR-7]: Feature Flag Control**  
**Description:** Environment variable toggles between old and new architecture for safe migration  
**Validation Method:**
```python
def test_feature_flag_controls_architecture():
    os.environ["USE_SIMPLIFIED_AGENT"] = "false"
    agent1 = QAAgent()
    assert agent1.use_new_architecture == False
    
    os.environ["USE_SIMPLIFIED_AGENT"] = "true"
    agent2 = QAAgent()
    assert agent2.use_new_architecture == True
```
**Acceptance Criterion:** Feature flag switches architecture without code changes

---

### NON-FUNCTIONAL REQUIREMENTS

**Performance:**
- **[NFR-P1]:** Single LLM analysis completes in <= 60s (p95 latency)
  - **Validation:** `assert analysis_duration_ms < 60000` in performance tests
- **[NFR-P2]:** Prompt token count <= 100,000 tokens (within Claude context window)
  - **Validation:** `assert estimate_tokens(prompt) <= 100000`
- **[NFR-P3]:** System handles playbooks up to 50 pages without chunking
  - **Validation:** Test with 50-page playbook, verify single prompt execution

**Reliability:**
- **[NFR-R1]:** Analysis success rate >= 95% over 100 test cases
  - **Validation:** Run test suite 100 times, `assert success_count >= 95`
- **[NFR-R2]:** LLM JSON parsing success rate >= 90%
  - **Validation:** Track JSON parse failures, alert if > 10%
- **[NFR-R3]:** Zero silent failures (all errors logged and raised)
  - **Validation:** Code review confirms no bare `except:` or ignored errors

**Observability:**
- **[NFR-O1]:** All LLM calls logged with request_id, model, latency, tokens
  - **Validation:** `assert "llm_call_completed" in log_entries`
- **[NFR-O2]:** Analysis stages logged with timing for performance debugging
  - **Validation:** Logs include `load_time_ms`, `prompt_build_time_ms`, `llm_call_time_ms`
- **[NFR-O3]:** Validation failures logged with specific error details
  - **Validation:** Log entries include `validation_errors: [list of specific errors]`

**Security:**
- **[NFR-S1]:** API keys loaded from environment, never hardcoded
  - **Validation:** `git grep "OPENROUTER_API_KEY.*=" returns no hardcoded keys
- **[NFR-S2]:** Logs sanitized to remove potential PII from playbook content
  - **Validation:** Logs contain playbook hash, not full playbook text
- **[NFR-S3]:** File paths validated to prevent directory traversal
  - **Validation:** `test_load_playbook_rejects_path_traversal()` passes

---

### CRITICAL CONSTRAINTS

**Absolute Prohibitions:**

❌ **NO Clinical Logic in Code**
- **Reason:** Medical decisions must come from clinical expertise (playbook + LLM), not code
- **Consequence:** If code contains regex for exams, normalization logic, or clinical validation → FAIL REVIEW
- **Validation:** `grep -r "audiometria|timpanometria|normalize.*exam|_extract.*fallback" src/agent_v2/` returns 0 matches

❌ **NO Specialty-Specific Code Paths**
- **Reason:** System must work identically for ORL, AVC, Pediatrics, etc.
- **Consequence:** If code contains `if specialty == "ORL":` logic → FAIL REVIEW
- **Validation:** `grep -r "if.*specialty|ORL|AVC" src/agent_v2/` returns 0 matches in logic files

❌ **NO Heuristic Quality Thresholds**
- **Reason:** LLM assesses quality, code doesn't second-guess
- **Consequence:** If code contains confidence thresholds, retry logic based on content → FAIL REVIEW
- **Validation:** No `if confidence < 0.8:` or `if len(exams) < 5:` patterns in code

❌ **NO Modification of Input Content**
- **Reason:** Agent is transparent pass-through, no transformations
- **Consequence:** If code normalizes, filters, or alters playbook/protocol → FAIL REVIEW
- **Validation:** Loaded content === file content (byte-for-byte)

**Mandatory Requirements:**

✅ **ALL Clinical Analysis from LLM**
- **Reason:** LLM has medical understanding, code doesn't
- **Validation:** Every clinical element in output traceable to LLM response
- **Verification:** Log shows `source=llm` for all clinical extractions

✅ **Comprehensive Single Prompt**
- **Reason:** One LLM call reduces complexity, latency, cost
- **Validation:** Pipeline uses exactly 1 LLM API call per analysis (log shows single `llm_call_completed`)
- **Exception:** Chunking allowed only if playbook > 50 pages

✅ **Schema Backward Compatibility**
- **Reason:** Downstream components (semantic_analyzer, report_generator) must work unchanged
- **Validation:** Integration tests with existing components pass
- **Verification:** `test_integration_with_semantic_analyzer()` and `test_integration_with_report_generator()` pass

✅ **Feature Flag Controlled**
- **Reason:** Safe migration requires ability to rollback instantly
- **Validation:** `USE_SIMPLIFIED_AGENT=false` reverts to legacy architecture
- **Verification:** Toggle feature flag, run test suite, both architectures pass

**Compatibility Requirements:**

**semantic_analyzer.py compatibility:**
- Must provide `playbook_data` dict with keys: `syndromes`, `exams`, `treatments`
- `exams` must include `normalized_name` field (even if same as `name`)
- Structure: `{"syndromes": [{"name": str}], "exams": [{"name": str, "normalized_name": str}]}`
- **Why:** semantic_analyzer.py expects this exact schema, will fail otherwise

**report_generator.py compatibility:**
- Must provide `coverage_analysis` dict with key: `score` (float 0-100)
- Must provide `improvement_suggestions` list of dicts with keys: `priority`, `description`
- **Why:** report_generator.py templates reference these exact keys

---

### ERROR HANDLING REQUIREMENTS

**Input Validation:**

```python
# Validate playbook file exists and is readable
def load_playbook(self, file_path: str) -> str:
    # Check: File exists
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Playbook not found: {file_path}")
    
    # Check: File is readable
    if not os.access(file_path, os.R_OK):
        raise PermissionError(f"Cannot read playbook: {file_path}")
    
    # Check: Path is safe (no directory traversal)
    if ".." in file_path or file_path.startswith("/"):
        raise ValueError(f"Invalid path (security): {file_path}")
    
    # On failure: Raise exception immediately
    # Log: File path, error type, timestamp
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        log.error(f"Failed to load playbook: {file_path}", exc_info=True)
        raise
    
    # Check: Content not empty
    if not content.strip():
        raise ValueError(f"Playbook is empty: {file_path}")
    
    log.info(f"Loaded playbook: {file_path}, size: {len(content)} bytes")
    return content

# Validate protocol JSON is well-formed
def load_protocol(self, file_path: str) -> dict:
    # Check: File exists and is readable (same as playbook)
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Protocol not found: {file_path}")
    
    # Check: Valid JSON
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            protocol = json.load(f)
    except json.JSONDecodeError as e:
        log.error(f"Invalid JSON in protocol: {file_path}, error: {e}")
        raise ValueError(f"Malformed JSON in protocol: {e}")
    
    # Check: Protocol not empty
    if not protocol:
        raise ValueError(f"Protocol is empty: {file_path}")
    
    log.info(f"Loaded protocol: {file_path}, nodes: {len(protocol.get('nodes', []))}")
    return protocol
```

**Processing Errors:**

```python
# Handle LLM API failures
def analyze(self, prompt: str, max_retries: int = 3) -> dict:
    for attempt in range(max_retries):
        try:
            response = self._call_llm_api(prompt)
            return self._parse_json_response(response)
        
        except requests.Timeout as e:
            # Catch: API timeout
            # Log: Attempt number, timeout duration, prompt hash
            log.warning(f"LLM API timeout (attempt {attempt+1}/{max_retries})", 
                       extra={"prompt_hash": hash(prompt), "timeout_s": 30})
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # Exponential backoff
            else:
                log.error("LLM API timeout after max retries")
                raise LLMError("LLM API timeout after 3 attempts")
        
        except requests.HTTPError as e:
            # Catch: Rate limiting (429), server errors (5xx)
            if e.response.status_code == 429:
                log.warning(f"Rate limited (attempt {attempt+1}/{max_retries})")
                if attempt < max_retries - 1:
                    time.sleep(5 * (attempt + 1))
                else:
                    raise LLMError("Rate limited after max retries")
            else:
                # Log: HTTP status, response body, prompt hash
                log.error(f"LLM API error: {e.response.status_code}", 
                         extra={"response": e.response.text[:500]})
                raise LLMError(f"LLM API error: {e.response.status_code}")
        
        except json.JSONDecodeError as e:
            # Catch: Malformed JSON in LLM response
            # Log: Response text preview, parse error location
            log.error(f"Malformed JSON from LLM: {e}", 
                     extra={"response_preview": response[:500]})
            
            # Attempt repair
            repaired = self._attempt_json_repair(response)
            if repaired:
                log.info("Successfully repaired malformed JSON")
                return repaired
            else:
                raise LLMError(f"Could not parse LLM response as JSON: {e}")
    
    # Return: Structured error dict (not exception for graceful degradation)
    # Do NOT: Silently return None or empty dict

def _attempt_json_repair(self, response: str) -> Optional[dict]:
    """Attempt to extract JSON from markdown code blocks or fix common issues"""
    # Strategy 1: Extract from markdown
    json_match = re.search(r'```json\s*(\{.*\})\s*```', response, re.DOTALL)
    if json_match:
        try:
            return json.loads(json_match.group(1))
        except json.JSONDecodeError:
            pass
    
    # Strategy 2: Fix common issues (trailing commas, etc)
    try:
        import json_repair
        return json_repair.loads(response)
    except:
        pass
    
    return None
```

**Failure Modes:**

**[FAILURE MODE 1]: LLM Returns Incomplete Response**  
**Detection:**
```python
def _validate_response_completeness(self, response: dict) -> tuple[bool, list[str]]:
    """Check if LLM response contains all required fields"""
    required_keys = [
        "clinical_extraction",
        "structural_analysis",
        "clinical_alignment",
        "recommendations",
        "safety_assessment",
        "metadata"
    ]
    
    missing = [key for key in required_keys if key not in response]
    return len(missing) == 0, missing
```
**Response:** Raise ValidationError with missing fields, do not continue
**Recovery:** Log error, return structured error response (not None)

**[FAILURE MODE 2]: LLM Hallucination of Non-Existent Elements**  
**Detection:**
```python
def _detect_hallucinations(self, response: dict, playbook: str) -> list[str]:
    """Flag elements in response not found in source playbook"""
    warnings = []
    
    for exam in response["clinical_extraction"]["exams"]:
        exam_name = exam["name"].lower()
        # Simple substring check (not perfect but catches obvious hallucinations)
        if exam_name not in playbook.lower():
            warnings.append(f"Potential hallucination: '{exam['name']}' not in playbook")
    
    return warnings
```
**Response:** Log warnings, include in metadata, do not fail analysis
**Recovery:** Continue with analysis but flag for human review

**[FAILURE MODE 3]: Playbook Exceeds Context Window**  
**Detection:**
```python
def build_qa_analysis_prompt(self, playbook: str, protocol: dict) -> Union[str, list[str]]:
    """Build prompt, chunking if necessary"""
    MAX_TOKENS = 100000
    PROMPT_OVERHEAD = 2000
    
    total_tokens = self._estimate_tokens(playbook) + self._estimate_tokens(json.dumps(protocol)) + PROMPT_OVERHEAD
    
    if total_tokens > MAX_TOKENS:
        log.warning(f"Playbook exceeds context limit ({total_tokens} tokens), chunking required")
        return self._build_chunked_prompts(playbook, protocol)
    else:
        return self._build_single_prompt(playbook, protocol)
```
**Response:** Implement chunking strategy (extract → analyze → synthesize)
**Recovery:** Multi-step analysis with smaller prompts

---

### VALIDATION REQUIREMENTS

**Pre-conditions:**

```python
# Before analysis execution:
def run_analysis(self, playbook_path: str, protocol_path: str) -> dict:
    # Pre-condition 1: Files exist
    assert os.path.exists(playbook_path), f"Playbook not found: {playbook_path}"
    assert os.path.exists(protocol_path), f"Protocol not found: {protocol_path}"
    
    # Pre-condition 2: Files are readable
    assert os.access(playbook_path, os.R_OK), f"Cannot read playbook: {playbook_path}"
    assert os.access(protocol_path, os.R_OK), f"Cannot read protocol: {protocol_path}"
    
    # Pre-condition 3: Paths are safe
    assert ".." not in playbook_path, "Invalid path (directory traversal)"
    assert ".." not in protocol_path, "Invalid path (directory traversal)"
    
    # Continue with analysis...
```

**Post-conditions:**

```python
# After LLM analysis:
def analyze(self, prompt: str) -> dict:
    response = self._call_llm_api(prompt)
    result = self._parse_json_response(response)
    
    # Post-condition 1: Response is valid JSON
    assert isinstance(result, dict), "LLM response must be dict"
    
    # Post-condition 2: Response has required schema
    validate_schema(result, EXPECTED_SCHEMA)
    
    # Post-condition 3: Clinical extraction not empty (unless playbook truly empty)
    if len(playbook) > 100:  # Non-trivial playbook
        assert len(result["clinical_extraction"]["syndromes"]) > 0 or \
               len(result["clinical_extraction"]["exams"]) > 0, \
               "LLM extracted no clinical elements from non-empty playbook"
    
    # Post-condition 4: Coverage score in valid range
    assert 0 <= result["clinical_alignment"]["coverage_score"] <= 100, \
           f"Invalid coverage score: {result['clinical_alignment']['coverage_score']}"
    
    return result
```

**Invariants:**

**[INV-1]:** Analysis always produces structured output (never None, never raw string)  
**[INV-2]:** All clinical elements in output traceable to LLM response (not code-generated)  
**[INV-3]:** Identical playbook+protocol inputs produce identical outputs (deterministic)  
**[INV-4]:** Code never modifies playbook or protocol content (transparent pass-through)

---

### LOGGING REQUIREMENTS

**Log Location:** `logs/qa_agent.log`  
**Log Format:** JSON (structured logging for parsing)

**Required Log Entries:**

```python
# On analysis start:
log.info({
    "timestamp": datetime.now().isoformat(),
    "event": "analysis_started",
    "architecture": "simplified",  # or "legacy"
    "context": {
        "playbook_path": playbook_path,
        "protocol_path": protocol_path,
        "playbook_size_bytes": os.path.getsize(playbook_path),
        "protocol_size_bytes": os.path.getsize(protocol_path)
    }
})

# On prompt construction:
log.debug({
    "timestamp": datetime.now().isoformat(),
    "event": "prompt_constructed",
    "context": {
        "prompt_hash": hashlib.md5(prompt.encode()).hexdigest(),
        "prompt_tokens": estimate_tokens(prompt),
        "template_version": "1.0",
        "sections_included": ["clinical_extraction", "structural_analysis", ...]
    }
})

# On LLM call:
log.info({
    "timestamp": datetime.now().isoformat(),
    "event": "llm_call_started",
    "context": {
        "model": "anthropic/claude-3.5-sonnet",
        "request_id": request_id,
        "estimated_tokens": 5000,
        "timeout_s": 60
    }
})

# On LLM response:
log.info({
    "timestamp": datetime.now().isoformat(),
    "event": "llm_call_completed",
    "context": {
        "request_id": request_id,
        "latency_ms": latency_ms,
        "tokens_used": tokens_used,
        "response_size_bytes": len(response),
        "cost_usd": estimated_cost
    },
    "outcome": "success"
})

# On validation:
log.info({
    "timestamp": datetime.now().isoformat(),
    "event": "response_validated",
    "context": {
        "schema_valid": True,
        "completeness_score": 1.0,
        "hallucination_warnings": len(warnings),
        "validation_errors": errors
    }
})

# On analysis complete:
log.info({
    "timestamp": datetime.now().isoformat(),
    "event": "analysis_completed",
    "context": {
        "total_duration_ms": total_ms,
        "stage_durations": {
            "load_ms": load_time,
            "prompt_build_ms": prompt_time,
            "llm_call_ms": llm_time,
            "validation_ms": validation_time
        },
        "entities_extracted": {
            "syndromes": count_syndromes,
            "exams": count_exams,
            "treatments": count_treatments
        },
        "coverage_score": coverage_score
    },
    "outcome": "success"
})

# On error:
log.error({
    "timestamp": datetime.now().isoformat(),
    "event": "analysis_failed",
    "context": {
        "playbook_path": playbook_path,
        "protocol_path": protocol_path,
        "stage": "llm_call",  # Which stage failed
        "error_type": "LLMTimeout",
        "error_message": str(error),
        "stack_trace": traceback.format_exc()
    },
    "outcome": "failure"
})
```

**Log Levels:**

- **DEBUG:** Prompt construction details, token counts, cache status
  - Use when: Need detailed information for debugging prompt engineering
- **INFO:** Analysis stages, LLM calls, validation results, successful completions
  - Use when: Tracking normal system operation, performance metrics
- **WARNING:** Low confidence scores, potential hallucinations, retries, degraded performance
  - Use when: System working but with non-critical issues requiring attention
- **ERROR:** LLM failures, validation failures, file errors, API errors
  - Use when: Operation failed but system still functional (recoverable errors)
- **CRITICAL:** System-level failures, configuration errors, unrecoverable states
  - Use when: System cannot continue operation (requires intervention)

---

### TESTING REQUIREMENTS

**Unit Tests:**

```python
def test_content_loader_loads_valid_playbook():
    """
    Validates: ContentLoader correctly reads playbook file without modification
    Given: Valid markdown playbook file at tests/data/sample_playbook.md
    When: load_playbook() called with file path
    Then: Returns string content exactly matching file contents
    """
    loader = ContentLoader()
    expected = open("tests/data/sample_playbook.md").read()
    actual = loader.load_playbook("tests/data/sample_playbook.md")
    
    assert isinstance(actual, str)
    assert actual == expected  # Byte-for-byte identical
    assert len(actual) > 0

def test_content_loader_raises_on_missing_playbook():
    """
    Validates: ContentLoader fails fast on missing file
    Given: Non-existent file path
    When: load_playbook() called
    Then: Raises FileNotFoundError with clear message
    """
    loader = ContentLoader()
    with pytest.raises(FileNotFoundError) as exc:
        loader.load_playbook("nonexistent.md")
    
    assert "Playbook not found" in str(exc.value)

def test_content_loader_loads_valid_protocol():
    """
    Validates: ContentLoader correctly parses JSON protocol
    Given: Valid JSON protocol file at tests/data/sample_protocol.json
    When: load_protocol() called with file path
    Then: Returns dict matching JSON structure
    """
    loader = ContentLoader()
    protocol = loader.load_protocol("tests/data/sample_protocol.json")
    
    assert isinstance(protocol, dict)
    assert "nodes" in protocol or "questions" in protocol  # Valid protocol structure

def test_content_loader_raises_on_malformed_json():
    """
    Validates: ContentLoader fails fast on invalid JSON
    Given: File with malformed JSON
    When: load_protocol() called
    Then: Raises ValueError with specific parse error
    """
    loader = ContentLoader()
    with pytest.raises(ValueError) as exc:
        loader.load_protocol("tests/data/malformed.json")
    
    assert "Invalid JSON" in str(exc.value) or "Malformed JSON" in str(exc.value)

def test_prompt_builder_creates_valid_prompt():
    """
    Validates: PromptBuilder generates well-formed comprehensive analysis prompt
    Given: Sample playbook content and protocol JSON
    When: build_qa_analysis_prompt() called
    Then: Returns prompt with all required analysis sections
    """
    builder = PromptBuilder()
    playbook = "# Test Playbook\n\nSyndrome: Test"
    protocol = {"nodes": [{"id": "1", "question": "Test?"}]}
    
    prompt = builder.build_qa_analysis_prompt(playbook, protocol)
    
    assert isinstance(prompt, str)
    assert "CLINICAL CONTENT EXTRACTION" in prompt
    assert "STRUCTURAL PROTOCOL ANALYSIS" in prompt
    assert "CLINICAL-PROTOCOL ALIGNMENT" in prompt
    assert "SAFETY VALIDATION" in prompt
    assert playbook in prompt  # Source content included
    assert json.dumps(protocol) in prompt

def test_prompt_builder_includes_output_schema():
    """
    Validates: PromptBuilder includes expected JSON schema in prompt
    Given: Standard inputs
    When: build_qa_analysis_prompt() called
    Then: Prompt contains OUTPUT STRUCTURE section with JSON schema
    """
    builder = PromptBuilder()
    prompt = builder.build_qa_analysis_prompt("playbook", {})
    
    assert "OUTPUT STRUCTURE" in prompt or "JSON schema" in prompt
    assert "clinical_extraction" in prompt
    assert "structural_analysis" in prompt

def test_llm_client_validates_response_schema():
    """
    Validates: LLMClient rejects malformed LLM responses immediately
    Given: Mock LLM returns incomplete JSON (missing required keys)
    When: analyze() called
    Then: Raises ValidationError with specific missing fields listed
    """
    client = LLMClient()
    client._call_llm_api = lambda p: '{"clinical_extraction": {}}'  # Incomplete
    
    with pytest.raises(ValidationError) as exc:
        client.analyze("test prompt")
    
    error_msg = str(exc.value)
    assert "Missing required keys" in error_msg or "Incomplete response" in error_msg

def test_llm_client_handles_malformed_json():
    """
    Validates: LLMClient attempts JSON repair before failing
    Given: Mock LLM returns JSON in markdown code block
    When: analyze() called
    Then: Successfully extracts and parses JSON from markdown
    """
    client = LLMClient()
    client._call_llm_api = lambda p: '```json\n{"clinical_extraction": {}}\n```'
    
    # Should extract JSON successfully
    result = client.analyze("test prompt")
    assert isinstance(result, dict)
    assert "clinical_extraction" in result

def test_qa_runner_handles_missing_playbook():
    """
    Validates: QARunner fails gracefully on missing input files
    Given: Playbook file doesn't exist
    When: run_analysis() called
    Then: Raises FileNotFoundError with clear error message
    """
    runner = SimplifiedQARunner()
    
    with pytest.raises(FileNotFoundError) as exc:
        runner.run_analysis("nonexistent.md", "protocol.json")
    
    assert "Playbook not found" in str(exc.value)

def test_qa_runner_handles_missing_protocol():
    """
    Validates: QARunner fails gracefully on missing protocol file
    Given: Protocol file doesn't exist
    When: run_analysis() called
    Then: Raises FileNotFoundError with clear error message
    """
    runner = SimplifiedQARunner()
    
    with pytest.raises(FileNotFoundError) as exc:
        runner.run_analysis("tests/data/sample_playbook.md", "nonexistent.json")
    
    assert "Protocol not found" in str(exc.value)

def test_schema_adapter_transforms_to_legacy():
    """
    Validates: SchemaAdapter correctly transforms new LLM output to legacy format
    Given: New LLM output with new schema structure
    When: to_legacy_format() called
    Then: Returns dict with exact legacy schema (playbook_data, coverage_analysis)
    """
    adapter = SchemaAdapter()
    
    new_output = {
        "clinical_extraction": {
            "syndromes": [{"name": "Test Syndrome", "confidence": 0.9}],
            "exams": [{"name": "Test Exam", "indication": "diagnosis"}]
        },
        "clinical_alignment": {
            "coverage_score": 75.5
        }
    }
    
    legacy = adapter.to_legacy_format(new_output)
    
    assert "playbook_data" in legacy
    assert "syndromes" in legacy["playbook_data"]
    assert "exams" in legacy["playbook_data"]
    # Critical: normalized_name field preserved for semantic_analyzer compatibility
    assert "normalized_name" in legacy["playbook_data"]["exams"][0]
    assert "coverage_analysis" in legacy
    assert "score" in legacy["coverage_analysis"]

def test_schema_adapter_preserves_normalized_name():
    """
    Validates: SchemaAdapter includes normalized_name for semantic_analyzer
    Given: New output with exam name
    When: to_legacy_format() called
    Then: Legacy format includes normalized_name field
    """
    adapter = SchemaAdapter()
    new_output = {
        "clinical_extraction": {
            "exams": [{"name": "Audiometry"}]
        }
    }
    
    legacy = adapter.to_legacy_format(new_output)
    exam = legacy["playbook_data"]["exams"][0]
    
    assert "normalized_name" in exam
    assert exam["normalized_name"] is not None
```

**Integration Tests:**

```python
def test_end_to_end_orl_protocol_analysis():
    """
    Validates: Complete analysis pipeline for ORL specialty
    Tests integration between: ContentLoader → PromptBuilder → LLMClient → OutputFormatter → SchemaAdapter
    Given: Real ORL playbook + protocol from tests/data/orl/
    When: run_analysis() executed
    Then: Returns complete analysis with all required fields, no errors
    """
    runner = SimplifiedQARunner(model="claude-sonnet-4")
    result = runner.run_analysis(
        "tests/data/orl/playbook.md",
        "tests/data/orl/protocol.json"
    )
    
    assert isinstance(result, dict)
    assert "clinical_extraction" in result
    assert "structural_analysis" in result
    assert "clinical_alignment" in result
    assert "recommendations" in result
    assert "safety_assessment" in result
    
    # Verify clinical extraction not empty
    assert len(result["clinical_extraction"]["syndromes"]) > 0 or \
           len(result["clinical_extraction"]["exams"]) > 0

def test_end_to_end_avc_protocol_analysis():
    """
    Validates: System works identically for different specialty (AVC)
    Tests integration between: Same pipeline as ORL test
    Given: Real AVC playbook + protocol from tests/data/avc/
    When: run_analysis() executed
    Then: Returns analysis with identical structure to ORL (specialty-agnostic)
    """
    runner = SimplifiedQARunner(model="claude-sonnet-4")
    result = runner.run_analysis(
        "tests/data/avc/playbook.md",
        "tests/data/avc/protocol.json"
    )
    
    # Same assertions as ORL test
    assert isinstance(result, dict)
    assert "clinical_extraction" in result
    # Verify identical code path (no specialty-specific logic executed)

def test_llm_response_compatible_with_semantic_analyzer():
    """
    Validates: LLM output (via adapter) compatible with existing semantic_analyzer
    Tests integration between: SimplifiedQARunner → SchemaAdapter → semantic_analyzer.py
    Given: LLM analysis output from new system
    When: semantic_analyzer.analyze_coverage() called with transformed output
    Then: Analysis completes successfully without schema errors
    """
    runner = SimplifiedQARunner()
    result = runner.run_analysis("tests/data/orl/playbook.md", "tests/data/orl/protocol.json")
    
    adapter = SchemaAdapter()
    legacy_format = adapter.to_legacy_format(result)
    
    # Should not raise errors
    from semantic_analyzer import analyze_coverage
    coverage = analyze_coverage(
        legacy_format["playbook_data"],
        legacy_format["coverage_analysis"]
    )
    
    assert coverage is not None
    assert "score" in coverage

def test_llm_response_compatible_with_report_generator():
    """
    Validates: LLM output (via adapter) compatible with existing report_generator
    Tests integration between: SimplifiedQARunner → SchemaAdapter → report_generator.py
    Given: LLM analysis output from new system
    When: report_generator.generate_report() called with transformed output
    Then: Report generates successfully with all expected sections
    """
    runner = SimplifiedQARunner()
    result = runner.run_analysis("tests/data/orl/playbook.md", "tests/data/orl/protocol.json")
    
    adapter = SchemaAdapter()
    legacy_format = adapter.to_legacy_format(result)
    
    # Should not raise errors
    from report_generator import generate_report
    report = generate_report(legacy_format)
    
    assert report is not None
    assert "coverage_score" in report or "Coverage Score" in report

def test_cli_integration_with_new_agent():
    """
    Validates: CLI interface works with new agent architecture via wrapper
    Tests integration between: run_qa_cli.py → QAAgent (wrapper) → SimplifiedQARunner
    Given: CLI invoked with test playbook + protocol, USE_SIMPLIFIED_AGENT=true
    When: CLI executes analysis command
    Then: Analysis completes successfully, results displayed correctly
    """
    import subprocess
    
    env = os.environ.copy()
    env["USE_SIMPLIFIED_AGENT"] = "true"
    
    result = subprocess.run([
        "python", "run_qa_cli.py",
        "--playbook", "tests/data/orl/playbook.md",
        "--protocol", "tests/data/orl/protocol.json",
        "--model", "claude-sonnet-4"
    ], capture_output=True, text=True, env=env)
    
    assert result.returncode == 0
    assert "coverage_score" in result.stdout.lower() or "analysis complete" in result.stdout.lower()

def test_feature_flag_toggles_architecture():
    """
    Validates: Feature flag successfully switches between old and new architecture
    Tests integration between: Environment variable → QAAgent initialization → Architecture selection
    Given: Feature flag set to false, then true
    When: QAAgent instantiated
    Then: Correct architecture selected (legacy vs simplified)
    """
    # Test legacy architecture
    os.environ["USE_SIMPLIFIED_AGENT"] = "false"
    agent1 = QAAgent()
    assert agent1.use_new_architecture == False
    
    # Test new architecture
    os.environ["USE_SIMPLIFIED_AGENT"] = "true"
    agent2 = QAAgent()
    assert agent2.use_new_architecture == True
    
    # Both should have same interface
    result1 = agent1.analyze_protocol("tests/data/orl/playbook.md", "tests/data/orl/protocol.json")
    result2 = agent2.analyze_protocol("tests/data/orl/playbook.md", "tests/data/orl/protocol.json")
    
    # Results should have same structure (even if values differ)
    assert set(result1.keys()) == set(result2.keys())
```

**Regression Tests:**

```python
def test_playbook_extraction_count_maintains_baseline():
    """
    Ensures: New system extracts at least as many entities as current system
    Given: Baseline test playbook with known entity counts from current system
    When: New system analyzes same playbook
    Then: Extracts >= baseline counts for syndromes, exams, treatments
    """
    # Run baseline with current system
    os.environ["USE_SIMPLIFIED_AGENT"] = "false"
    legacy_agent = QAAgent()
    legacy_result = legacy_agent.analyze_protocol(
        "tests/data/baseline_playbook.md",
        "tests/data/baseline_protocol.json"
    )
    
    legacy_counts = {
        "syndromes": len(legacy_result["playbook_data"]["syndromes"]),
        "exams": len(legacy_result["playbook_data"]["exams"]),
        "treatments": len(legacy_result["playbook_data"]["treatments"])
    }
    
    # Run new system
    os.environ["USE_SIMPLIFIED_AGENT"] = "true"
    new_agent = QAAgent()
    new_result = new_agent.analyze_protocol(
        "tests/data/baseline_playbook.md",
        "tests/data/baseline_protocol.json"
    )
    
    new_counts = {
        "syndromes": len(new_result["playbook_data"]["syndromes"]),
        "exams": len(new_result["playbook_data"]["exams"]),
        "treatments": len(new_result["playbook_data"]["treatments"])
    }
    
    # New system must extract at least as many entities
    assert new_counts["syndromes"] >= legacy_counts["syndromes"], \
           f"Syndrome extraction regressed: {new_counts['syndromes']} < {legacy_counts['syndromes']}"
    assert new_counts["exams"] >= legacy_counts["exams"], \
           f"Exam extraction regressed: {new_counts['exams']} < {legacy_counts['exams']}"
    assert new_counts["treatments"] >= legacy_counts["treatments"], \
           f"Treatment extraction regressed: {new_counts['treatments']} < {legacy_counts['treatments']}"

def test_protocol_validation_error_detection_preserved():
    """
    Ensures: New system catches same JSON errors as current system
    Given: Test protocol with known syntax and logic errors
    When: New system analyzes protocol
    Then: Reports all errors that current system reports
    """
    # Protocol with known errors
    test_protocol = {
        "nodes": [
            {"id": "1", "question": "Test?", "next": "999"},  # References non-existent node
            {"id": "2"}  # Missing required field
        ]
    }
    
    # Both systems should detect these errors
    os.environ["USE_SIMPLIFIED_AGENT"] = "false"
    legacy_agent = QAAgent()
    legacy_result = legacy_agent.analyze_protocol("tests/data/sample_playbook.md", test_protocol)
    
    os.environ["USE_SIMPLIFIED_AGENT"] = "true"
    new_agent = QAAgent()
    new_result = new_agent.analyze_protocol("tests/data/sample_playbook.md", test_protocol)
    
    # Verify new system catches same errors
    legacy_errors = set(legacy_result["structural_analysis"]["logic_issues"])
    new_errors = set(new_result["structural_analysis"]["logic_issues"])
    
    assert new_errors >= legacy_errors, "New system missing error detections"

def test_report_format_maintains_compatibility():
    """
    Ensures: Generated reports maintain existing format for downstream tools
    Given: Standard test protocol analyzed by both systems
    When: Reports generated from results
    Then: Report structures match (same sections, same field names)
    """
    from report_generator import generate_report
    
    # Generate report from legacy system
    os.environ["USE_SIMPLIFIED_AGENT"] = "false"
    legacy_agent = QAAgent()
    legacy_result = legacy_agent.analyze_protocol("tests/data/orl/playbook.md", "tests/data/orl/protocol.json")
    legacy_report = generate_report(legacy_result)
    
    # Generate report from new system
    os.environ["USE_SIMPLIFIED_AGENT"] = "true"
    new_agent = QAAgent()
    new_result = new_agent.analyze_protocol("tests/data/orl/playbook.md", "tests/data/orl/protocol.json")
    new_report = generate_report(new_result)
    
    # Reports should have same structure
    assert set(legacy_report.keys()) == set(new_report.keys()), \
           "Report structure changed, breaking compatibility"

def test_performance_not_degraded():
    """
    Ensures: New system completes analysis within acceptable time
    Given: Baseline performance metrics from current system
    When: New system analyzes same protocols 10 times
    Then: p95 completion time <= baseline p95 * 1.5 (allow 50% degradation max)
    """
    import time
    
    # Measure legacy performance
    os.environ["USE_SIMPLIFIED_AGENT"] = "false"
    legacy_agent = QAAgent()
    legacy_times = []
    
    for _ in range(10):
        start = time.time()
        legacy_agent.analyze_protocol("tests/data/orl/playbook.md", "tests/data/orl/protocol.json")
        legacy_times.append(time.time() - start)
    
    legacy_p95 = sorted(legacy_times)[int(0.95 * len(legacy_times))]
    
    # Measure new system performance
    os.environ["USE_SIMPLIFIED_AGENT"] = "true"
    new_agent = QAAgent()
    new_times = []
    
    for _ in range(10):
        start = time.time()
        new_agent.analyze_protocol("tests/data/orl/playbook.md", "tests/data/orl/protocol.json")
        new_times.append(time.time() - start)
    
    new_p95 = sorted(new_times)[int(0.95 * len(new_times))]
    
    # Allow up to 50% performance degradation
    assert new_p95 <= legacy_p95 * 1.5, \
           f"Performance degraded too much: {new_p95:.2f}s > {legacy_p95 * 1.5:.2f}s"
```

**Test Data:**

- **Location:** `tests/data/`
- **Format:** 
  - `sample_playbook.md` - Small test playbook (5 pages, 3 syndromes, 5 exams)
  - `baseline_playbook.md` - Baseline playbook with known entity counts
  - `large_playbook.md` - 50-page playbook for context window testing
  - `orl/playbook.md` + `orl/protocol.json` - Real ORL specialty data
  - `avc/playbook.md` + `avc/protocol.json` - Real AVC specialty data
  - `malformed.json` - Invalid JSON for error handling tests
- **Requirements:**
  - Cover at least 3 different medical specialties
  - Include edge cases (empty playbook, huge protocol, malformed JSON)
  - Include known-good baseline data with documented entity counts

**All Tests Must:**
- Be deterministic (no random behavior, no flaky tests)
- Run in < 5s each for unit tests (integration tests can be slower)
- Be independent (no test order dependencies, no shared state)
- Clean up resources (close files, reset environment variables)
- Have descriptive names and docstrings explaining what/why/how

---

### IMPLEMENTATION GUIDANCE

**File Structure:**

```
src/
├── agent_v2/                    # New simplified architecture (Phase 1)
│   ├── __init__.py
│   ├── loader.py                # ContentLoader class - load files without interpretation
│   ├── prompt_builder.py        # PromptBuilder class - construct super prompt
│   ├──llm_client.py            # LLMClient class - LLM API communication
│   ├── qa_runner.py             # SimplifiedQARunner class - orchestrate workflow
│   └── output/
│       ├── __init__.py
│       ├── formatter.py         # OutputFormatter class - structure LLM response
│       ├── validator.py         # ResponseValidator class - validate LLM output
│       └── schema_adapter.py    # SchemaAdapter class - transform to legacy format
│
├── agent/                       # Existing architecture (preserved for Phase 1)
│   ├── __init__.py
│   ├── qa_agent.py             # Legacy QAAgent - kept for feature flag
│   └── ... (other legacy modules)
│
├── config/
│   ├── __init__.py
│   ├── prompts.py              # Super prompt templates
│   └── feature_flags.py        # Feature flag configuration
│
└── tests/
    ├── unit/
    │   ├── test_loader.py
    │   ├── test_prompt_builder.py
    │   ├── test_llm_client.py
    │   ├── test_validator.py
    │   └── test_schema_adapter.py
    ├── integration/
    │   ├── test_end_to_end.py
    │   ├── test_downstream_compatibility.py
    │   └── test_cli_integration.py
    ├── regression/
    │   ├── test_extraction_baseline.py
    │   ├── test_error_detection.py
    │   └── test_performance.py
    └── data/
        ├── sample_playbook.md
        ├── baseline_playbook.md
        ├── orl/ (playbook.md + protocol.json)
        └── avc/ (playbook.md + protocol.json)
```

**Code Organization:**

```python
# src/agent_v2/loader.py
"""
Content loading without any clinical interpretation.
Loads playbook (markdown/PDF) and protocol (JSON) as raw data.
"""

class ContentLoader:
    """Simple file loading with validation"""
    
    def load_playbook(self, file_path: str) -> str:
        """
        Load playbook content as raw text.
        
        Args:
            file_path: Path to playbook file (markdown or PDF)
            
        Returns:
            Raw playbook content as string
            
        Raises:
            FileNotFoundError: If file doesn't exist
            PermissionError: If file not readable
            ValueError: If file is empty or path invalid
        """
        # Validate file exists
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Playbook not found: {file_path}")
        
        # Validate path safety (no directory traversal)
        if ".." in file_path:
            raise ValueError(f"Invalid path (security): {file_path}")
        
        # Load content
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Validate not empty
        if not content.strip():
            raise ValueError(f"Playbook is empty: {file_path}")
        
        log.info(f"Loaded playbook: {file_path}, size: {len(content)} bytes")
        return content
    
    def load_protocol(self, file_path: str) -> dict:
        """
        Load protocol JSON as dictionary.
        
        Args:
            file_path: Path to protocol JSON file
            
        Returns:
            Protocol as dictionary
            
        Raises:
            FileNotFoundError: If file doesn't exist
            ValueError: If JSON is malformed or empty
        """
        # Validate file exists
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Protocol not found: {file_path}")
        
        # Load and parse JSON
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                protocol = json.load(f)
        except json.JSONDecodeError as e:
            raise ValueError(f"Malformed JSON in protocol: {e}")
        
        # Validate not empty
        if not protocol:
            raise ValueError(f"Protocol is empty: {file_path}")
        
        log.info(f"Loaded protocol: {file_path}, nodes: {len(protocol.get('nodes', []))}")
        return protocol
```

```python
# src/agent_v2/prompt_builder.py
"""
Unified analysis prompt construction.
Builds comprehensive super prompt for LLM analysis.
"""

from config.prompts import SUPER_PROMPT_TEMPLATE, OUTPUT_SCHEMA

class PromptBuilder:
    """Build comprehensive analysis prompts for LLM"""
    
    def build_qa_analysis_prompt(self, playbook_content: str, protocol_json: dict) -> str:
        """
        Create comprehensive QA analysis prompt.
        
        Args:
            playbook_content: Raw playbook text
            protocol_json: Protocol dictionary
            
        Returns:
            Complete prompt string ready for LLM
        """
        # Format protocol as pretty JSON
        protocol_formatted = json.dumps(protocol_json, indent=2, ensure_ascii=False)
        
        # Substitute into template
        prompt = SUPER_PROMPT_TEMPLATE.format(
            playbook_content=playbook_content,
            protocol_json=protocol_formatted,
            output_schema=OUTPUT_SCHEMA
        )
        
        # Log prompt metadata (not full content to save log space)
        token_count = self._estimate_tokens(prompt)
        prompt_hash = hashlib.md5(prompt.encode()).hexdigest()
        
        log.debug(f"Prompt constructed: hash={prompt_hash}, tokens={token_count}")
        
        return prompt
    
    def _estimate_tokens(self, text: str) -> int:
        """Rough token estimation (1 token ≈ 4 chars for Claude)"""
        return len(text) // 4
```

```python
# src/agent_v2/llm_client.py
"""
LLM API communication without clinical logic.
Handles API calls, retries, JSON parsing, validation.
"""

from typing import Optional
import requests
import time

class LLMClient:
    """Simple LLM communication client"""
    
    def __init__(self, model: str = "anthropic/claude-3.5-sonnet"):
        self.api_key = os.getenv("OPENROUTER_API_KEY")
        if not self.api_key:
            raise ValueError("OPENROUTER_API_KEY environment variable not set")
        
        self.model = model
        self.api_url = "https://openrouter.ai/api/v1/chat/completions"
    
    def analyze(self, prompt: str, max_retries: int = 3) -> dict:
        """
        Send analysis prompt to LLM, return structured response.
        
        Args:
            prompt: Complete analysis prompt
            max_retries: Number of retry attempts on failure
            
        Returns:
            Structured analysis as dictionary
            
        Raises:
            LLMError: If LLM call fails after retries
            ValidationError: If response doesn't match expected schema
        """
        request_id = str(uuid.uuid4())
        
        log.info({
            "event": "llm_call_started",
            "request_id": request_id,
            "model": self.model,
            "estimated_tokens": len(prompt) // 4
        })
        
        # Retry logic with exponential backoff
        for attempt in range(max_retries):
            try:
                start_time = time.time()
                response_text = self._call_llm_api(prompt)
                latency_ms = int((time.time() - start_time) * 1000)
                
                log.info({
                    "event": "llm_call_completed",
                    "request_id": request_id,
                    "latency_ms": latency_ms,
                    "outcome": "success"
                })
                
                # Parse and validate response
                result = self._parse_json_response(response_text)
                self._validate_response(result)
                
                return result
                
            except requests.Timeout:
                log.warning(f"LLM timeout (attempt {attempt+1}/{max_retries})")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)  # Exponential backoff
                else:
                    raise LLMError("LLM API timeout after max retries")
            
            except requests.HTTPError as e:
                if e.response.status_code == 429:  # Rate limit
                    log.warning(f"Rate limited (attempt {attempt+1}/{max_retries})")
                    if attempt < max_retries - 1:
                        time.sleep(5 * (attempt + 1))
                    else:
                        raise LLMError("Rate limited after max retries")
                else:
                    log.error(f"LLM API error: {e.response.status_code}")
                    raise LLMError(f"LLM API error: {e.response.status_code}")
    
    def _call_llm_api(self, prompt: str) -> str:
        """Make actual API call to OpenRouter"""
        response = requests.post(
            self.api_url,
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            },
            json={
                "model": self.model,
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.1,  # Low temperature for consistency
                "max_tokens": 16000
            },
            timeout=60
        )
        
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    
    def _parse_json_response(self, response: str) -> dict:
        """Parse JSON from LLM response, attempting repair if needed"""
        # Strategy 1: Direct parsing
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            pass
        
        # Strategy 2: Extract from markdown code blocks
        json_match = re.search(r'```json\s*(\{.*\})\s*```', response, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except json.JSONDecodeError:
                pass
        
        # Failed all strategies
        log.error(f"Could not parse JSON from LLM response: {response[:500]}")
        raise LLMError("Malformed JSON in LLM response")
    
    def _validate_response(self, response: dict):
        """Validate response has required structure"""
        required_keys = [
            "clinical_extraction",
            "structural_analysis",
            "clinical_alignment",
            "recommendations",
            "safety_assessment",
            "metadata"
        ]
        
        missing = [key for key in required_keys if key not in response]
        if missing:
            raise ValidationError(f"LLM response missing required keys: {missing}")
```

```python
# src/agent_v2/qa_runner.py
"""
Analysis workflow orchestration.
Coordinates loading, prompting, LLM analysis, formatting.
"""

from .loader import ContentLoader
from .prompt_builder import PromptBuilder
from .llm_client import LLMClient
from .output.formatter import OutputFormatter
from .output.schema_adapter import SchemaAdapter

class SimplifiedQARunner:
    """Orchestrate QA analysis workflow without clinical logic"""
    
    def __init__(self, model: str = "anthropic/claude-3.5-sonnet"):
        self.loader = ContentLoader()
        self.prompt_builder = PromptBuilder()
        self.llm_client = LLMClient(model=model)
        self.formatter = OutputFormatter()
        self.adapter = SchemaAdapter()
    
    def run_analysis(self, playbook_path: str, protocol_path: str) -> dict:
        """
        Run complete QA analysis using LLM only.
        
        Args:
            playbook_path: Path to playbook file
            protocol_path: Path to protocol JSON
            
        Returns:
            Complete analysis in legacy format (for compatibility)
            
        Raises:
            FileNotFoundError: If input files missing
            ValueError: If input files invalid
            LLMError: If LLM analysis fails
        """
        log.info({
            "event": "analysis_started",
            "architecture": "simplified",
            "playbook_path": playbook_path,
            "protocol_path": protocol_path
        })
        
        start_time = time.time()
        
        try:
            # Step 1: Load content (no interpretation)
            load_start = time.time()
            playbook = self.loader.load_playbook(playbook_path)
            protocol = self.loader.load_protocol(protocol_path)
            load_time_ms = int((time.time() - load_start) * 1000)
            
            # Step 2: Build prompt
            prompt_start = time.time()
            prompt = self.prompt_builder.build_qa_analysis_prompt(playbook, protocol)
            prompt_time_ms = int((time.time() - prompt_start) * 1000)
            
            # Step 3: Get LLM analysis
            llm_start = time.time()
            analysis = self.llm_client.analyze(prompt)
            llm_time_ms = int((time.time() - llm_start) * 1000)
            
            # Step 4: Format output
            format_start = time.time()
            formatted = self.formatter.format_analysis_output(analysis)
            format_time_ms = int((time.time() - format_start) * 1000)
            
            # Step 5: Transform to legacy schema for compatibility
            legacy_format = self.adapter.to_legacy_format(formatted)
            
            total_time_ms = int((time.time() - start_time) * 1000)
            
            log.info({
                "event": "analysis_completed",
                "total_duration_ms": total_time_ms,
                "stage_durations": {
                    "load_ms": load_time_ms,
                    "prompt_build_ms": prompt_time_ms,
                    "llm_call_ms": llm_time_ms,
                    "format_ms": format_time_ms
                },
                "entities_extracted": {
                    "syndromes": len(formatted["clinical_extraction"]["syndromes"]),
                    "exams": len(formatted["clinical_extraction"]["exams"]),
                    "treatments": len(formatted["clinical_extraction"]["treatments"])
                },
                "coverage_score": formatted["clinical_alignment"]["coverage_score"],
                "outcome": "success"
            })
            
            return legacy_format
            
        except Exception as e:
            log.error({
                "event": "analysis_failed",
                "playbook_path": playbook_path,
                "protocol_path": protocol_path,
                "error_type": type(e).__name__,
                "error_message": str(e),
                "outcome": "failure"
            }, exc_info=True)
            raise
```

**Patterns to Follow:**

**[PATTERN 1]: Fail-Fast Validation**
- When to use: All input validation, file loading, schema checking
- How to implement:
```python
# Validate inputs immediately, don't defer errors
def load_playbook(self, file_path: str) -> str:
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Playbook not found: {file_path}")  # Fail immediately
    # Don't return None or empty string on error
```

**[PATTERN 2]: Structured Logging**
- When to use: All significant events (analysis start/end, LLM calls, errors)
- How to implement:
```python
# Use structured JSON logging for parsing
log.info({
    "event": "llm_call_completed",
    "request_id": request_id,
    "latency_ms": latency_ms,
    "outcome": "success"
})
# Not: log.info(f"LLM call completed in {latency_ms}ms")
```

**[PATTERN 3]: Separation of Concerns**
- When to use: All module design
- How to implement:
```python
# Each class has single responsibility
class ContentLoader:  # Only loads files
class PromptBuilder:  # Only builds prompts
class LLMClient:      # Only calls LLM
class QARunner:       # Only orchestrates
# No class does clinical interpretation
```

**[PATTERN 4]: Explicit Error Context**
- When to use: All error handling
- How to implement:
```python
# Include context in error messages
raise FileNotFoundError(f"Playbook not found: {file_path}")
# Not: raise FileNotFoundError("File not found")

# Log full context on errors
log.error({
    "event": "analysis_failed",
    "playbook_path": playbook_path,  # Context
    "error_type": type(e).__name__,  # Error type
    "error_message": str(e)          # Error details
}, exc_info=True)
```

**Patterns to Avoid:**

**[ANTI-PATTERN 1]: Silent Failures**
- Why problematic: Errors propagate, causing cryptic failures later
- Alternative:
```python
# BAD:
try:
    result = risky_operation()
except:
    pass  # Silent failure

# GOOD:
try:
    result = risky_operation()
except Exception as e:
    log.error(f"Operation failed: {e}", exc_info=True)
    raise  # Re-raise, don't swallow
```

**[ANTI-PATTERN 2]: Code-Based Clinical Logic**
- Why problematic: Violates separation of concerns, unmaintainable
- Alternative:
```python
# BAD:
if "audiometria" in exam_name.lower():
    normalized = "Audiometry"  # Code making medical decisions

# GOOD:
# Let LLM normalize in response:
# {"name": "audiometria", "normalized_name": "Audiometry"}
```

**[ANTI-PATTERN 3]: Overly Broad Exception Handling**
- Why problematic: Catches errors you don't know how to handle
- Alternative:
```python
# BAD:
try:
    do_something()
except Exception:  # Too broad
    pass

# GOOD:
try:
    do_something()
except FileNotFoundError as e:  # Specific exception
    handle_missing_file(e)
except ValueError as e:  # Another specific exception
    handle_invalid_value(e)
# Let other exceptions propagate
```

---

### ACCEPTANCE CRITERIA

**Checklist for Completion:**

- [ ] All functional requirements implemented (FR-1 through FR-7)
- [ ] All non-functional requirements met (performance, reliability, observability)
- [ ] All constraints respected (no clinical logic, no specialty-specific code)
- [ ] All error handling implemented (input validation, processing errors, failure modes)
- [ ] All validations implemented (pre-conditions, post-conditions, invariants)
- [ ] All logging implemented (structured JSON logs for all events)
- [ ] All unit tests written and passing (100% of unit test suite)
- [ ] All integration tests passing (semantic_analyzer, report_generator compatibility)
- [ ] All regression tests passing (extraction, error detection, performance)
- [ ] Code reviewed against specification (no deviations without justification)
- [ ] Documentation updated (README, architecture docs, prompt docs)

**Measurable Success Criteria:**

**[CRITERION 1]: Zero Clinical Heuristics in Code**
- **Measurement:** `grep -r "regex.*audiometria|normalize.*exam|_extract.*fallback|timpanometria" src/agent_v2/`
- **Expected Value:** 0 matches

**[CRITERION 2]: Single LLM Call Per Analysis**
- **Measurement:** Count "llm_call_completed" events in logs for single analysis
- **Expected Value:** Exactly 1 (exception: chunking for large playbooks)

**[CRITERION 3]: Schema Compatibility Maintained**
- **Measurement:** `pytest tests/integration/test_downstream_compatibility.py -v`
- **Expected Value:** All tests pass (0 failures)

**[CRITERION 4]: Specialty-Agnostic Operation**
- **Measurement:** Compare code paths for ORL vs AVC vs Pediatrics analyses
- **Expected Value:** Identical execution path (verified in logs)

**[CRITERION 5]: Performance Acceptable**
- **Measurement:** p95 latency from performance regression test
- **Expected Value:** <= 60 seconds

**[CRITERION 6]: Analysis Quality Maintained**
- **Measurement:** Entity extraction count vs baseline (regression test)
- **Expected Value:** >= 90% of baseline counts

**[CRITERION 7]: Error Detection Preserved**
- **Measurement:** Number of protocol errors caught vs baseline
- **Expected Value:** >= 100% of baseline errors

**[CRITERION 8]: Test Coverage Adequate**
- **Measurement:** `pytest --cov=src/agent_v2 --cov-report=term`
- **Expected Value:** >= 80% code coverage

**[CRITERION 9]: Feature Flag Works**
- **Measurement:** Toggle USE_SIMPLIFIED_AGENT, verify architecture switch
- **Expected Value:** Both settings produce valid results, no errors

**[CRITERION 10]: Logging Complete**
- **Measurement:** Verify logs contain all required events (analysis_started, llm_call_completed, etc.)
- **Expected Value:** All required log events present for test analysis

---

### FILES TO MODIFY

| File | Changes | Risk | Backup Required |
|------|---------|------|-----------------|
| (none in Phase 1) | Phase 1 creates new files only, no modifications | LOW | No |

**Phase 1 creates only NEW files:**
- `src/agent_v2/*.py` (all new)
- `src/config/prompts.py` (new)
- `tests/unit/*.py` (new)
- `tests/integration/*.py` (new)

**Phase 2 will modify:**
- `src/agent/qa_agent.py` - Add wrapper for compatibility
- `run_qa_cli.py` - Add feature flag support

---

### DEPENDENCIES

**Required (Already Installed):**
- `requests` - LLM API calls
- `pytest` - Testing framework
- `python-dotenv` - Environment variable loading

**To Install:**
```bash
# Optional: JSON repair library
pip install json-repair==0.7.0

# Optional: Token estimation (if using chunking)
pip install tiktoken==0.5.2
```

**Modified:**
- None in Phase 1

---

### ROLLBACK PLAN

**If Implementation Fails:**
1. Delete `src/agent_v2/` directory
2. Remove test files in `tests/unit/`, `tests/integration/` for agent_v2
3. Verify existing system still works: `pytest tests/ -v`
4. No impact on production (Phase 1 is isolated)

**If Tests Fail:**
- Debug approach: Run tests individually to isolate failure
- Fix or revert decision criteria:
  - If unit test fails → Fix implementation (low risk)
  - If integration test fails → Fix schema compatibility (medium risk)
  - If regression test fails → May need prompt engineering (revisit super prompt)
  - If >50% of tests fail → Revert and redesign

**Rollback Triggers:**
- Cannot achieve 80% test coverage after reasonable effort
- Integration tests reveal fundamental incompatibility with downstream components
- Performance >3x worse than baseline (unacceptable degradation)
- LLM JSON parsing success rate <70% (too unreliable)

---

### IMPLEMENTATION CHECKLIST

**Pre-Implementation:**
- [ ] Read entire specification thoroughly
- [ ] Understand all constraints (especially no clinical logic)
- [ ] Review existing `semantic_analyzer.py` and `report_generator.py` to understand schema expectations
- [ ] Prepare test data (ensure ORL, AVC playbooks + protocols available)
- [ ] Create feature branch: `git checkout -b feature/simplified-qa-agent`

**During Implementation:**
- [ ] Follow Phase 1 file structure exactly (create `agent_v2/` directory)
- [ ] Implement validations first (fail-fast on invalid inputs)
- [ ] Add structured logging as you go (every significant event)
- [ ] Write tests alongside code (TDD approach recommended)
- [ ] Run tests frequently: `pytest tests/unit/ -v`
- [ ] Verify zero clinical logic: `grep -r "audiometria|normalize" src/agent_v2/`

**Post-Implementation:**
- [ ] All unit tests pass: `pytest tests/unit/ -v`
- [ ] All integration tests pass: `pytest tests/integration/ -v`
- [ ] All regression tests pass: `pytest tests/regression/ -v`
- [ ] Code coverage >= 80%: `pytest --cov=src/agent_v2`
- [ ] Code review against specification (self-review checklist)
- [ ] Documentation updated: README, architecture docs
- [ ] Acceptance criteria verified (all 10 measurable criteria met)

---

### CRITICAL REMINDERS

**This Implementation:**
- **MUST NOT contain any clinical logic** (no regex for exams, no normalization, no validation)
- **MUST maintain backward compatibility** (downstream components work unchanged)
- **MUST use single LLM call** (no multi-stage extraction pipeline)
- **MUST be specialty-agnostic** (identical code for ORL, AVC, Pediatrics)
- **MUST fail-fast on errors** (no silent failures, all errors logged and raised)

**Common Pitfalls:**
- **Pitfall 1**: Adding "just one small heuristic" for quality → Violates core principle
  - How to avoid: Trust LLM completely, resist urge to "fix" outputs with code
- **Pitfall 2**: Modifying playbook/protocol content before passing to LLM → Violates transparency
  - How to avoid: Pass content unchanged, let LLM handle interpretation
- **Pitfall 3**: Creating specialty-specific logic "temporarily" → Technical debt accumulates
  - How to avoid: Make system work for all specialties from day 1
- **Pitfall 4**: Swallowing exceptions for "better user experience" → Masks bugs
  - How to avoid: Always log and re-raise, fail loudly

**If You Encounter:**
- **LLM returns unexpected format**: Enhance prompt instructions, don't add code workarounds
- **Analysis quality seems low**: Improve prompt, don't add heuristics
- **Performance too slow**: Optimize prompt size or implement chunking, don't skip validation
- **Tests reveal schema incompatibility**: Add transformation in SchemaAdapter, don't modify downstream components yet

---

### IMPLEMENTATION ESTIMATE

**Complexity:** HIGH (Complete new architecture, but well-specified)  
**Estimated Time:** 3-5 days for Phase 1 (new modules + tests)  
**Risk Level:** MEDIUM (new code isolated, no integration yet)

**Confidence Level:** HIGH  
**Confidence Factors:**
- Specification is detailed and comprehensive
- Phase 1 is isolated (no modifications to existing system)
- Clear acceptance criteria and test requirements
- Well-defined rollback plan
- Similar architectures successfully implemented before

---

## QA SIGN-OFF

**Specification Status:** APPROVED FOR PHASE 1 IMPLEMENTATION

**Conditions for Approval:**
1. Phase 1 must be implemented in isolation (new `agent_v2/` directory, no modifications to existing code)
2. All 10 measurable acceptance criteria must be met before proceeding to Phase 2
3. Super prompt must be documented in `config/prompts.py` with clear versioning
4. Test suite must achieve >= 80% code coverage
5. Performance regression test must show p95 latency <= 60s

**Recommendation to PM:**  
This specification addresses all concerns raised in mandate. Phased implementation with validation gates mitigates risk of complete architecture replacement in production healthcare system. Recommend proceeding with Phase 1, with explicit go/no-go decision point before Phase 2 based on measurable Phase 1 outcomes.

**Recommendation to Software Engineer:**  
Focus on Phase 1 first - create completely isolated new architecture without touching existing code. This allows safe experimentation and validation before integration. Prioritize comprehensive prompt engineering (in `config/prompts.py`) as this will be the "brain" of the new system. Resist all temptation to add clinical logic to code - if LLM output quality is insufficient, enhance the prompt, not the code. Run tests continuously during development to catch schema incompatibilities early.

**Follow-up Required:**
1. After Phase 1 complete: Run parallel validation comparing old vs new architecture on 20+ test cases
2. Document any prompt iterations needed to achieve quality parity
3. Measure actual performance metrics and compare to estimates
4. Report any fundamental blockers to PM before proceeding to Phase 2

**QA Engineer:** Claude  
**Audit Date:** 2025-11-29T03:15:00Z  
**Specification Version:** 1.0 - Phase 1 (Foundation + Parallel Validation)